{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890a5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d470d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score, classification_report, pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602e7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9443005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e994a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = [2012, 2013]\n",
    "val_years = [2014]\n",
    "\n",
    "train_meta_df = pd.DataFrame()\n",
    "val_meta_df = pd.DataFrame()\n",
    "\n",
    "for ty in train_years:\n",
    "    for i in [1, 2]:\n",
    "        train_meta_df = pd.concat([train_meta_df, pd.read_csv(\"./data/doc_features/{year}_{part}_master_metadata.csv\".format(year=ty, part=i))])\n",
    "\n",
    "for ty in val_years:\n",
    "    for i in [1, 2]:\n",
    "        val_meta_df = pd.concat([val_meta_df, pd.read_csv(\"./data/doc_features/{year}_{part}_master_metadata.csv\".format(year=ty, part=i))])\n",
    "\n",
    "train_meta_df = train_meta_df[(train_meta_df['SUESCORE'] <= -0.5) | (train_meta_df['SUESCORE'] >= 0.5)].reset_index(drop=True)\n",
    "val_meta_df = val_meta_df[(val_meta_df['SUESCORE'] <= -0.5) | (val_meta_df['SUESCORE'] >= 0.5)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2874d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 1280\n"
     ]
    }
   ],
   "source": [
    "test_meta_df = val_meta_df.sample(frac=0.5, random_state=seed)\n",
    "test_meta_df = test_meta_df.reset_index(drop=True)\n",
    "\n",
    "val_meta_df = val_meta_df.drop(test_meta_df.index)\n",
    "val_meta_df = val_meta_df.reset_index(drop=True)\n",
    "\n",
    "print(len(val_meta_df), len(test_meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b9e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_embeddings(valid_transcriptids, model_name, cls_type, years=[2012,2013]):\n",
    "    dirname = \"./data/doc_features/{}_filtered/\".format(model_name)\n",
    "\n",
    "    if cls_type not in [\"X_cls\", \"X_mean\"]:\n",
    "        raise Exception(\"data type in valid\")\n",
    "    \n",
    "    X_list = []\n",
    "    \n",
    "    for ty in years:\n",
    "        for part in [1, 2]:\n",
    "            filename = \"transcript_componenttext_{}_{}_cls_mean.npz\".format(ty, part)\n",
    "\n",
    "            path = os.path.join(dirname, filename)\n",
    "\n",
    "            data = np.load(path)\n",
    "\n",
    "            X = data[cls_type]\n",
    "            tids = data[\"transcriptids\"]\n",
    "            tids_int = tids.astype(np.int64)\n",
    "\n",
    "            mask_ids = np.isin(tids_int, valid_transcriptids)\n",
    "\n",
    "            X_filt = X[mask_ids]\n",
    "            # tids_filt = tids[mask_ids]\n",
    "\n",
    "            X_list.append(X_filt)\n",
    "\n",
    "    return np.vstack(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140e9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_train_embs = gather_embeddings(train_meta_df.transcriptid, \"llama3_3b_cls\", \"X_mean\", years=train_years)\n",
    "qwen_train_embs = gather_embeddings(train_meta_df.transcriptid, \"qwen_4b_cls\", \"X_mean\", years=train_years)\n",
    "gemma_train_embs = gather_embeddings(train_meta_df.transcriptid, \"gemma_2b_cls\", \"X_mean\", years=train_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac886b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_val_embs = gather_embeddings(val_meta_df.transcriptid, \"llama3_3b_cls\", \"X_mean\", years=val_years)\n",
    "qwen_val_embs = gather_embeddings(val_meta_df.transcriptid, \"qwen_4b_cls\", \"X_mean\", years=val_years)\n",
    "gemma_val_embs = gather_embeddings(val_meta_df.transcriptid, \"gemma_2b_cls\", \"X_mean\", years=val_years)\n",
    "\n",
    "llama3_test_embs = gather_embeddings(test_meta_df.transcriptid, \"llama3_3b_cls\", \"X_mean\", years=val_years)\n",
    "qwen_test_embs = gather_embeddings(test_meta_df.transcriptid, \"qwen_4b_cls\", \"X_mean\", years=val_years)\n",
    "gemma_test_embs = gather_embeddings(test_meta_df.transcriptid, \"gemma_2b_cls\", \"X_mean\", years=val_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8b99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(gemma_train_embs)\n",
    "gemma_train_aligned = scaler.transform(gemma_train_embs)\n",
    "gemma_val_aligned = scaler.transform(gemma_val_embs)\n",
    "gemma_test_aligned = scaler.transform(gemma_test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955962fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_meta_df['label']\n",
    "y_val = val_meta_df['label']\n",
    "y_test = test_meta_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563da45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_transformation(X_train, target_train, X_test, target_test, X_val=None, use_skl=True, reg=10.0):\n",
    "    # train T: llama -> Gemma\n",
    "    scaler_X = StandardScaler().fit(X_train)\n",
    "    scaler_Y = StandardScaler().fit(target_train)\n",
    "\n",
    "    X_tr = scaler_X.transform(X_train)\n",
    "    Y_tr = scaler_Y.transform(target_train)\n",
    "    X_te = scaler_X.transform(X_test)\n",
    "    Y_te = scaler_Y.transform(target_test)\n",
    "\n",
    "    if X_val is not None:\n",
    "        X_va = scaler_X.transform(X_val)\n",
    "        \n",
    "    alpha = reg\n",
    "    d_X = X_tr.shape[1]\n",
    "\n",
    "    if not use_skl:\n",
    "        I = np.eye(d_X)\n",
    "        # compute W_closed:\n",
    "        W_closed = np.linalg.inv(X_tr.T @ X_tr + alpha * I) @ X_tr.T @ Y_tr\n",
    "\n",
    "        # Map test embeddings and inverse-transform:\n",
    "        Y_tr_from_transform_scaled = X_tr @ W_closed\n",
    "        \n",
    "        if X_val is not None:\n",
    "            Y_val_from_transform_scaled = X_va @ W_closed\n",
    "\n",
    "        Y_pred_scaled = X_te @ W_closed\n",
    "        Y_pred = scaler_Y.inverse_transform(Y_pred_scaled)\n",
    "\n",
    "        # Evaluate (e.g. MSE or cosine similarity)\n",
    "        mse = np.mean((Y_pred - scaler_Y.inverse_transform(Y_te))**2)\n",
    "        print(f\"Closed-form ridge MSE: {mse:.4f}\")\n",
    "        print(np.diag(cosine_similarity(Y_pred_scaled, Y_te)).mean())\n",
    "\n",
    "        if X_val is not None:\n",
    "            return W_closed, Y_tr_from_transform_scaled, Y_val_from_transform_scaled, Y_pred_scaled\n",
    "        else:\n",
    "            return W_closed, Y_tr_from_transform_scaled, Y_pred_scaled\n",
    "        \n",
    "    else:\n",
    "        # 3b) Using sklearn.Ridge:\n",
    "        # Note: sklearnâ€™s Ridge solves for each output dimension jointly when Y is 2D.\n",
    "        model = Ridge(alpha=alpha, fit_intercept=False, solver=\"auto\")# intercept is already handled by StandardScaler\n",
    "        model.fit(X_tr, Y_tr)\n",
    "        W_sklearn = model.coef_.T   \n",
    "\n",
    "        Y_tr_from_transform_scaled = model.predict(X_tr)\n",
    "\n",
    "        if X_val is not None:\n",
    "            Y_val_from_transform_scaled = model.predict(X_va)\n",
    "        \n",
    "        Y_pred_scaled = model.predict(X_te)\n",
    "\n",
    "        Y_pred = scaler_Y.inverse_transform(Y_pred_scaled)\n",
    "        mse = np.mean((Y_pred - scaler_Y.inverse_transform(Y_te))**2)\n",
    "        print(f\"sklearn.Ridge MSE: {mse:.4f}\")\n",
    "        print(np.diag(cosine_similarity(Y_pred_scaled, Y_te)).mean())\n",
    "\n",
    "        if X_val is not None:\n",
    "            return model, Y_tr_from_transform_scaled, Y_val_from_transform_scaled, Y_pred_scaled\n",
    "        else:\n",
    "            return model, Y_tr_from_transform_scaled, Y_pred_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f087379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-form ridge MSE: 0.1197\n",
      "sklearn.Ridge MSE:      0.1197\n"
     ]
    }
   ],
   "source": [
    "# train T: llama -> Gemma\n",
    "scaler_X = StandardScaler().fit(llama3_train_embs)\n",
    "scaler_Y = StandardScaler().fit(gemma_train_embs)\n",
    "\n",
    "X_tr = scaler_X.transform(llama3_train_embs)\n",
    "Y_tr = scaler_Y.transform(gemma_train_embs)\n",
    "X_te = scaler_X.transform(llama3_test_embs)\n",
    "Y_te = scaler_Y.transform(gemma_test_embs)\n",
    "\n",
    "\n",
    "alpha = 10.0\n",
    "d_llama = X_tr.shape[1]\n",
    "I = np.eye(d_llama)\n",
    "# compute W_closed:\n",
    "W_closed = np.linalg.inv(X_tr.T @ X_tr + alpha * I) @ X_tr.T @ Y_tr\n",
    "\n",
    "# Map test embeddings and inverse-transform:\n",
    "Y_pred_closed = scaler_Y.inverse_transform(X_te @ W_closed)\n",
    "\n",
    "# Evaluate (e.g. MSE or cosine similarity)\n",
    "mse_closed = np.mean((Y_pred_closed - scaler_Y.inverse_transform(Y_te))**2)\n",
    "print(f\"Closed-form ridge MSE: {mse_closed:.4f}\")\n",
    "\n",
    "# 3b) Using sklearn.Ridge:\n",
    "# Note: sklearnâ€™s Ridge solves for each output dimension jointly when Y is 2D.\n",
    "model = Ridge(alpha=alpha, fit_intercept=False, solver=\"auto\")# intercept is already handled by StandardScaler\n",
    "model.fit(X_tr, Y_tr)\n",
    "W_sklearn = model.coef_.T   # shape (d_llama, d_gemma)\n",
    "\n",
    "Y_pred_skl = scaler_Y.inverse_transform(model.predict(X_te))\n",
    "mse_skl = np.mean((Y_pred_skl - scaler_Y.inverse_transform(Y_te))**2)\n",
    "print(f\"sklearn.Ridge MSE:      {mse_skl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7984564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. cosine sim: 0.8950016481689836\n"
     ]
    }
   ],
   "source": [
    "# Y_te_scaled = scaler_Y.transform(gemma_test_embs)\n",
    "Y_pred_scaled = X_te @ W_closed\n",
    "# compute pairwise cosines for each i\n",
    "cosines = np.diag(cosine_similarity(Y_pred_scaled, Y_te))\n",
    "print(\"Avg. cosine sim:\", cosines.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3286e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. cosine sim: 0.8950016\n"
     ]
    }
   ],
   "source": [
    "Y_pred_scaled = model.predict(X_te)\n",
    "cosines = np.diag(cosine_similarity(Y_pred_scaled, Y_te))\n",
    "print(\"Avg. cosine sim:\", cosines.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.Ridge MSE: 0.0954\n",
      "0.91327083\n",
      "Closed-form ridge MSE: 0.1153\n",
      "0.8903838461753889\n"
     ]
    }
   ],
   "source": [
    "# trans_llama_gemma, llama_train_aligned, llama_val_aligned, llama_test_aligned = fit_linear_transformation(llama3_train_embs, gemma_train_embs, llama3_test_embs, gemma_test_embs, llama3_val_embs, use_skl=True, reg=200)\n",
    "# trans_qwen_gemma, qwen_train_aligned, qwen_val_aligned, qwen_test_aligned = fit_linear_transformation(qwen_train_embs, gemma_train_embs, qwen_test_embs, gemma_test_embs, qwen_val_embs, use_skl=False, reg=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0454a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.Ridge MSE: 0.1474\n",
      "0.8751319\n",
      "Closed-form ridge MSE: 0.1668\n",
      "0.8507743229369137\n"
     ]
    }
   ],
   "source": [
    "trans_llama_gemma, llama_train_aligned, llama_val_aligned, llama_test_aligned = fit_linear_transformation(llama3_train_embs, gemma_train_embs, llama3_test_embs, gemma_test_embs, llama3_val_embs, use_skl=True, reg=2)\n",
    "trans_qwen_gemma, qwen_train_aligned, qwen_val_aligned, qwen_test_aligned = fit_linear_transformation(qwen_train_embs, gemma_train_embs, qwen_test_embs, gemma_test_embs, qwen_val_embs, use_skl=False, reg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd288be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8751319\n",
      "0.8507743229369137\n",
      "0.8261375792973121\n"
     ]
    }
   ],
   "source": [
    "print(np.diag(cosine_similarity(gemma_test_aligned, llama_test_aligned)).mean())\n",
    "print(np.diag(cosine_similarity(qwen_test_aligned, gemma_test_aligned)).mean())\n",
    "print(np.diag(cosine_similarity(qwen_test_aligned, llama_test_aligned)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "732cbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8727312\n",
      "0.8465191930255955\n",
      "0.8219528728637322\n"
     ]
    }
   ],
   "source": [
    "print(np.diag(cosine_similarity(gemma_val_aligned, llama_val_aligned)).mean())\n",
    "print(np.diag(cosine_similarity(qwen_val_aligned, gemma_val_aligned)).mean())\n",
    "print(np.diag(cosine_similarity(qwen_val_aligned, llama_val_aligned)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a28c9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_std_analysis(gemma_emb, qwen_trans_emb, llama_trans_emb, n_clusters=50):\n",
    "    \"\"\"\n",
    "    Quantifies uniqueness of transformed embeddings within Gemma-defined clusters\n",
    "    \n",
    "    Args:\n",
    "        gemma_emb: Original Gemma embeddings (n_samples, dim)\n",
    "        qwen_trans_emb: Qwen embeddings transformed to Gemma space (n_samples, dim)\n",
    "        llama_trans_emb: Llama embeddings transformed to Gemma space (n_samples, dim)\n",
    "        n_clusters: Number of clusters to create\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing uniqueness metrics and cluster statistics\n",
    "    \"\"\"\n",
    "    # Cluster in original Gemma space\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(gemma_emb)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    gemma_stds, qwen_stds, llama_stds = [], [], []\n",
    "    uniqueness_scores_qwen, uniqueness_scores_llama = [], []\n",
    "    \n",
    "    for c in range(n_clusters):\n",
    "        # Get indices for current cluster\n",
    "        cluster_idx = np.where(cluster_labels == c)[0]\n",
    "        if len(cluster_idx) < 5:  # Skip small clusters\n",
    "            continue\n",
    "            \n",
    "        # Get cluster center\n",
    "        center = cluster_centers[c]\n",
    "        \n",
    "        # Calculate distances to cluster center\n",
    "        gemma_dists = pairwise_distances(gemma_emb[cluster_idx], [center])\n",
    "        qwen_dists = pairwise_distances(qwen_trans_emb[cluster_idx], [center])\n",
    "        llama_dists = pairwise_distances(llama_trans_emb[cluster_idx], [center])\n",
    "        \n",
    "        # Compute standard deviations\n",
    "        gemma_std = np.std(gemma_dists)\n",
    "        qwen_std = np.std(qwen_dists)\n",
    "        llama_std = np.std(llama_dists)\n",
    "        \n",
    "        # Calculate uniqueness scores\n",
    "        uniqueness_qwen = max(0, 1 - (qwen_std / gemma_std))\n",
    "        uniqueness_llama = max(0, 1 - (llama_std / gemma_std))\n",
    "        \n",
    "        # Store results\n",
    "        gemma_stds.append(gemma_std)\n",
    "        qwen_stds.append(qwen_std)\n",
    "        llama_stds.append(llama_std)\n",
    "        uniqueness_scores_qwen.append(uniqueness_qwen)\n",
    "        uniqueness_scores_llama.append(uniqueness_llama)\n",
    "    \n",
    "    # Compute cross-model divergence\n",
    "    cross_divergence = 1 - np.mean([\n",
    "        np.dot(qwen_trans_emb[i], llama_trans_emb[i]) / \n",
    "        (np.linalg.norm(qwen_trans_emb[i]) * np.linalg.norm(llama_trans_emb[i]))\n",
    "        for i in range(len(gemma_emb))\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"mean_uniqueness_qwen\": np.mean(uniqueness_scores_qwen),\n",
    "        \"mean_uniqueness_llama\": np.mean(uniqueness_scores_llama),\n",
    "        \"std_ratio_qwen\": np.mean(qwen_stds) / np.mean(gemma_stds),\n",
    "        \"std_ratio_llama\": np.mean(llama_stds) / np.mean(gemma_stds),\n",
    "        \"cross_model_divergence\": cross_divergence,\n",
    "        \"per_cluster\": {\n",
    "            \"gemma_std\": gemma_stds,\n",
    "            \"qwen_std\": qwen_stds,\n",
    "            \"llama_std\": llama_stds,\n",
    "            \"uniqueness_qwen\": uniqueness_scores_qwen,\n",
    "            \"uniqueness_llama\": uniqueness_scores_llama\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59fa121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen uniqueness: 0.024\n",
      "Llama uniqueness: 0.049\n",
      "Qwen std ratio: 1.098\n",
      "Llama std ratio: 1.027\n",
      "Cross-model divergence: 0.174\n"
     ]
    }
   ],
   "source": [
    "results = cluster_std_analysis(gemma_test_aligned, llama_test_aligned, qwen_test_aligned)\n",
    "    \n",
    "print(f\"Qwen uniqueness: {results['mean_uniqueness_qwen']:.3f}\")\n",
    "print(f\"Llama uniqueness: {results['mean_uniqueness_llama']:.3f}\")\n",
    "print(f\"Qwen std ratio: {results['std_ratio_qwen']:.3f}\")\n",
    "print(f\"Llama std ratio: {results['std_ratio_llama']:.3f}\")\n",
    "print(f\"Cross-model divergence: {results['cross_model_divergence']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6633c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingConcatDataset(Dataset):\n",
    "    def __init__(self, aligned_embs: List[NDArray[np.float32]], labels):\n",
    "        \n",
    "        self.concat_embs = np.concat(aligned_embs, axis=1)\n",
    "        self.num_models = len(aligned_embs)\n",
    "        self.emb_dim = aligned_embs[0].shape[1]\n",
    "        self.concat_emb_dim = self.num_models * self.emb_dim\n",
    "        self.labels = labels\n",
    "\n",
    "        assert self.concat_embs.shape[0] == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concat_embs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.concat_embs[index]\n",
    "        y = self.labels[index] \n",
    "\n",
    "        x_tensor = torch.from_numpy(x)\n",
    "        y_tensor = torch.tensor(y)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb3bb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aligned_embs= [gemma_train_aligned, llama_train_aligned, qwen_train_aligned]\n",
    "# val_aligned_embs= [gemma_val_aligned, llama_val_aligned, qwen_val_aligned]\n",
    "# test_aligned_embs= [gemma_test_aligned, llama_test_aligned, qwen_test_aligned]\n",
    "\n",
    "# train_ds = EmbeddingConcatDataset(train_aligned_embs, y_train)\n",
    "# val_ds = EmbeddingConcatDataset(val_aligned_embs, y_val)\n",
    "# test_ds = EmbeddingConcatDataset(test_aligned_embs, y_test)\n",
    "\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def to_tensor_dataset(X, y):\n",
    "    Xt = torch.from_numpy(X).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return TensorDataset(Xt, yt)\n",
    "\n",
    "train_aligned_embs = np.array([gemma_train_aligned, llama_train_aligned, qwen_train_aligned]).mean(axis=0)\n",
    "val_aligned_embs = np.array([gemma_val_aligned, llama_val_aligned, qwen_val_aligned]).mean(axis=0)\n",
    "test_aligned_embs = np.array([gemma_test_aligned, llama_test_aligned, qwen_test_aligned]).mean(axis=0)\n",
    "\n",
    "train_ds = to_tensor_dataset(train_aligned_embs, y_train.to_numpy())\n",
    "val_ds = to_tensor_dataset(val_aligned_embs, y_val.to_numpy())\n",
    "test_ds = to_tensor_dataset(test_aligned_embs, y_test.to_numpy())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee7680e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 0.], shape=(4920,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best C (inverse reg. strength): 0.006\n",
      "CV ROC AUC: 0.6585952175685856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.19      0.08      0.11       270\n",
      "         1.0       0.79      0.91      0.85      1010\n",
      "\n",
      "    accuracy                           0.74      1280\n",
      "   macro avg       0.49      0.50      0.48      1280\n",
      "weighted avg       0.66      0.74      0.69      1280\n",
      "\n",
      "Test ROC AUC: 0.494987165383205\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# X_mean, l2 0.005\n",
    "param_grid = {\"logisticregression__C\": [0.0001, 0.006, 0.1]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    # StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(train_aligned_embs, y_train.to_numpy())\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(test_aligned_embs)[:, 1]\n",
    "y_pred       = best_clf.predict(test_aligned_embs)\n",
    "\n",
    "print(classification_report(y_test.to_numpy(), y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test.to_numpy(), y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d2d7e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train Loss: 0.7511  Val Loss: 0.7380\n",
      "Epoch  2  Train Loss: 0.7388  Val Loss: 0.7248\n",
      "Epoch  3  Train Loss: 0.7324  Val Loss: 0.7151\n",
      "Epoch  4  Train Loss: 0.7216  Val Loss: 0.7156\n",
      "Epoch  5  Train Loss: 0.7119  Val Loss: 0.7026\n",
      "Epoch  6  Train Loss: 0.7044  Val Loss: 0.6991\n",
      "Epoch  7  Train Loss: 0.7061  Val Loss: 0.6925\n",
      "Epoch  8  Train Loss: 0.7018  Val Loss: 0.6888\n",
      "Epoch  9  Train Loss: 0.6935  Val Loss: 0.6897\n",
      "Epoch 10  Train Loss: 0.6894  Val Loss: 0.6773\n",
      "Epoch 11  Train Loss: 0.6830  Val Loss: 0.6800\n",
      "Epoch 12  Train Loss: 0.6775  Val Loss: 0.6757\n",
      "Epoch 13  Train Loss: 0.6775  Val Loss: 0.6727\n",
      "Epoch 14  Train Loss: 0.6682  Val Loss: 0.6654\n",
      "Epoch 15  Train Loss: 0.6642  Val Loss: 0.6637\n",
      "Epoch 16  Train Loss: 0.6608  Val Loss: 0.6630\n",
      "Epoch 17  Train Loss: 0.6591  Val Loss: 0.6530\n",
      "Epoch 18  Train Loss: 0.6612  Val Loss: 0.6497\n",
      "Epoch 19  Train Loss: 0.6540  Val Loss: 0.6498\n",
      "Epoch 20  Train Loss: 0.6496  Val Loss: 0.6493\n",
      "Epoch 21  Train Loss: 0.6479  Val Loss: 0.6458\n",
      "Epoch 22  Train Loss: 0.6398  Val Loss: 0.6425\n",
      "Epoch 23  Train Loss: 0.6363  Val Loss: 0.6375\n",
      "Epoch 24  Train Loss: 0.6291  Val Loss: 0.6337\n",
      "Epoch 25  Train Loss: 0.6226  Val Loss: 0.6332\n",
      "Epoch 26  Train Loss: 0.6244  Val Loss: 0.6360\n",
      "Epoch 27  Train Loss: 0.6179  Val Loss: 0.6264\n",
      "Epoch 28  Train Loss: 0.6189  Val Loss: 0.6219\n",
      "Epoch 29  Train Loss: 0.6107  Val Loss: 0.6184\n",
      "Epoch 30  Train Loss: 0.6096  Val Loss: 0.6160\n",
      "Epoch 31  Train Loss: 0.6093  Val Loss: 0.6187\n",
      "Epoch 32  Train Loss: 0.6021  Val Loss: 0.6160\n",
      "Epoch 33  Train Loss: 0.6056  Val Loss: 0.6146\n",
      "Epoch 34  Train Loss: 0.6005  Val Loss: 0.6045\n",
      "Epoch 35  Train Loss: 0.6002  Val Loss: 0.6063\n",
      "Epoch 36  Train Loss: 0.5944  Val Loss: 0.5991\n",
      "Epoch 37  Train Loss: 0.5923  Val Loss: 0.6064\n",
      "Epoch 38  Train Loss: 0.5855  Val Loss: 0.6007\n",
      "Epoch 39  Train Loss: 0.5858  Val Loss: 0.5987\n",
      "Epoch 40  Train Loss: 0.5884  Val Loss: 0.5958\n",
      "Epoch 41  Train Loss: 0.5800  Val Loss: 0.5886\n",
      "Epoch 42  Train Loss: 0.5827  Val Loss: 0.5908\n",
      "Epoch 43  Train Loss: 0.5796  Val Loss: 0.5882\n",
      "Epoch 44  Train Loss: 0.5751  Val Loss: 0.5880\n",
      "Epoch 45  Train Loss: 0.5706  Val Loss: 0.5832\n",
      "Epoch 46  Train Loss: 0.5696  Val Loss: 0.5840\n",
      "Epoch 47  Train Loss: 0.5665  Val Loss: 0.5851\n",
      "Epoch 48  Train Loss: 0.5659  Val Loss: 0.5832\n",
      "Epoch 49  Train Loss: 0.5633  Val Loss: 0.5784\n",
      "Epoch 50  Train Loss: 0.5642  Val Loss: 0.5746\n",
      "Epoch 51  Train Loss: 0.5595  Val Loss: 0.5733\n",
      "Epoch 52  Train Loss: 0.5561  Val Loss: 0.5696\n",
      "Epoch 53  Train Loss: 0.5519  Val Loss: 0.5705\n",
      "Epoch 54  Train Loss: 0.5482  Val Loss: 0.5676\n",
      "Epoch 55  Train Loss: 0.5491  Val Loss: 0.5663\n",
      "Epoch 56  Train Loss: 0.5469  Val Loss: 0.5642\n",
      "Epoch 57  Train Loss: 0.5441  Val Loss: 0.5628\n",
      "Epoch 58  Train Loss: 0.5436  Val Loss: 0.5670\n",
      "Epoch 59  Train Loss: 0.5447  Val Loss: 0.5659\n",
      "Epoch 60  Train Loss: 0.5421  Val Loss: 0.5606\n",
      "Epoch 61  Train Loss: 0.5390  Val Loss: 0.5589\n",
      "Epoch 62  Train Loss: 0.5365  Val Loss: 0.5555\n",
      "Epoch 63  Train Loss: 0.5361  Val Loss: 0.5574\n",
      "Epoch 64  Train Loss: 0.5358  Val Loss: 0.5516\n",
      "Epoch 65  Train Loss: 0.5339  Val Loss: 0.5537\n",
      "Epoch 66  Train Loss: 0.5300  Val Loss: 0.5567\n",
      "Epoch 67  Train Loss: 0.5303  Val Loss: 0.5485\n",
      "Epoch 68  Train Loss: 0.5289  Val Loss: 0.5480\n",
      "Epoch 69  Train Loss: 0.5275  Val Loss: 0.5470\n",
      "Epoch 70  Train Loss: 0.5238  Val Loss: 0.5490\n",
      "Epoch 71  Train Loss: 0.5221  Val Loss: 0.5449\n",
      "Epoch 72  Train Loss: 0.5207  Val Loss: 0.5444\n",
      "Epoch 73  Train Loss: 0.5223  Val Loss: 0.5429\n",
      "Epoch 74  Train Loss: 0.5240  Val Loss: 0.5448\n",
      "Epoch 75  Train Loss: 0.5129  Val Loss: 0.5411\n",
      "Epoch 76  Train Loss: 0.5166  Val Loss: 0.5366\n",
      "Epoch 77  Train Loss: 0.5164  Val Loss: 0.5383\n",
      "Epoch 78  Train Loss: 0.5151  Val Loss: 0.5353\n",
      "Epoch 79  Train Loss: 0.5160  Val Loss: 0.5385\n",
      "Epoch 80  Train Loss: 0.5125  Val Loss: 0.5383\n",
      "Epoch 81  Train Loss: 0.5124  Val Loss: 0.5374\n",
      "Epoch 82  Train Loss: 0.5078  Val Loss: 0.5338\n",
      "Epoch 83  Train Loss: 0.5050  Val Loss: 0.5311\n",
      "Epoch 84  Train Loss: 0.5064  Val Loss: 0.5343\n",
      "Epoch 85  Train Loss: 0.5026  Val Loss: 0.5394\n",
      "Epoch 86  Train Loss: 0.5054  Val Loss: 0.5296\n",
      "Epoch 87  Train Loss: 0.5026  Val Loss: 0.5330\n",
      "Epoch 88  Train Loss: 0.5024  Val Loss: 0.5299\n",
      "Epoch 89  Train Loss: 0.5029  Val Loss: 0.5304\n",
      "Epoch 90  Train Loss: 0.5004  Val Loss: 0.5251\n",
      "Epoch 91  Train Loss: 0.5002  Val Loss: 0.5254\n",
      "Epoch 92  Train Loss: 0.4971  Val Loss: 0.5259\n",
      "Epoch 93  Train Loss: 0.4955  Val Loss: 0.5250\n",
      "Epoch 94  Train Loss: 0.4944  Val Loss: 0.5275\n",
      "Epoch 95  Train Loss: 0.4968  Val Loss: 0.5289\n",
      "Epoch 96  Train Loss: 0.4949  Val Loss: 0.5239\n",
      "Epoch 97  Train Loss: 0.4955  Val Loss: 0.5258\n",
      "Epoch 98  Train Loss: 0.4927  Val Loss: 0.5245\n",
      "Epoch 99  Train Loss: 0.4916  Val Loss: 0.5243\n",
      "Epoch 100  Train Loss: 0.4927  Val Loss: 0.5220\n",
      "Epoch 101  Train Loss: 0.4879  Val Loss: 0.5226\n",
      "Epoch 102  Train Loss: 0.4887  Val Loss: 0.5213\n",
      "Epoch 103  Train Loss: 0.4896  Val Loss: 0.5193\n",
      "Epoch 104  Train Loss: 0.4912  Val Loss: 0.5205\n",
      "Epoch 105  Train Loss: 0.4878  Val Loss: 0.5183\n",
      "Epoch 106  Train Loss: 0.4851  Val Loss: 0.5211\n",
      "Epoch 107  Train Loss: 0.4831  Val Loss: 0.5195\n",
      "Epoch 108  Train Loss: 0.4820  Val Loss: 0.5182\n",
      "Epoch 109  Train Loss: 0.4854  Val Loss: 0.5179\n",
      "Epoch 110  Train Loss: 0.4846  Val Loss: 0.5180\n",
      "Epoch 111  Train Loss: 0.4827  Val Loss: 0.5196\n",
      "Epoch 112  Train Loss: 0.4812  Val Loss: 0.5166\n",
      "Epoch 113  Train Loss: 0.4826  Val Loss: 0.5173\n",
      "Epoch 114  Train Loss: 0.4791  Val Loss: 0.5158\n",
      "Epoch 115  Train Loss: 0.4808  Val Loss: 0.5143\n",
      "Epoch 116  Train Loss: 0.4808  Val Loss: 0.5159\n",
      "Epoch 117  Train Loss: 0.4753  Val Loss: 0.5171\n",
      "Epoch 118  Train Loss: 0.4769  Val Loss: 0.5145\n",
      "Epoch 119  Train Loss: 0.4766  Val Loss: 0.5164\n",
      "Epoch 120  Train Loss: 0.4772  Val Loss: 0.5116\n",
      "Epoch 121  Train Loss: 0.4738  Val Loss: 0.5137\n",
      "Epoch 122  Train Loss: 0.4739  Val Loss: 0.5127\n",
      "Epoch 123  Train Loss: 0.4773  Val Loss: 0.5144\n",
      "Epoch 124  Train Loss: 0.4729  Val Loss: 0.5126\n",
      "Epoch 125  Train Loss: 0.4713  Val Loss: 0.5107\n",
      "Epoch 126  Train Loss: 0.4697  Val Loss: 0.5106\n",
      "Epoch 127  Train Loss: 0.4741  Val Loss: 0.5098\n",
      "Epoch 128  Train Loss: 0.4705  Val Loss: 0.5122\n",
      "Epoch 129  Train Loss: 0.4684  Val Loss: 0.5121\n",
      "Epoch 130  Train Loss: 0.4698  Val Loss: 0.5101\n",
      "Epoch 131  Train Loss: 0.4648  Val Loss: 0.5126\n",
      "Epoch 132  Train Loss: 0.4694  Val Loss: 0.5099\n",
      "Epoch 133  Train Loss: 0.4669  Val Loss: 0.5108\n",
      "Epoch 134  Train Loss: 0.4662  Val Loss: 0.5106\n",
      "Epoch 135  Train Loss: 0.4634  Val Loss: 0.5096\n",
      "Epoch 136  Train Loss: 0.4677  Val Loss: 0.5099\n",
      "Epoch 137  Train Loss: 0.4702  Val Loss: 0.5074\n",
      "Epoch 138  Train Loss: 0.4612  Val Loss: 0.5093\n",
      "Epoch 139  Train Loss: 0.4626  Val Loss: 0.5086\n",
      "Epoch 140  Train Loss: 0.4671  Val Loss: 0.5090\n",
      "Epoch 141  Train Loss: 0.4641  Val Loss: 0.5087\n",
      "Epoch 142  Train Loss: 0.4603  Val Loss: 0.5078\n",
      "Epoch 143  Train Loss: 0.4638  Val Loss: 0.5076\n",
      "Epoch 144  Train Loss: 0.4615  Val Loss: 0.5083\n",
      "Epoch 145  Train Loss: 0.4604  Val Loss: 0.5051\n",
      "Epoch 146  Train Loss: 0.4602  Val Loss: 0.5077\n",
      "Epoch 147  Train Loss: 0.4577  Val Loss: 0.5064\n",
      "Epoch 148  Train Loss: 0.4598  Val Loss: 0.5097\n",
      "Epoch 149  Train Loss: 0.4623  Val Loss: 0.5071\n",
      "Epoch 150  Train Loss: 0.4572  Val Loss: 0.5055\n",
      "Epoch 151  Train Loss: 0.4583  Val Loss: 0.5065\n",
      "Epoch 152  Train Loss: 0.4550  Val Loss: 0.5080\n",
      "Epoch 153  Train Loss: 0.4605  Val Loss: 0.5064\n",
      "Epoch 154  Train Loss: 0.4605  Val Loss: 0.5065\n",
      "Epoch 155  Train Loss: 0.4618  Val Loss: 0.5065\n",
      "Epoch 156  Train Loss: 0.4588  Val Loss: 0.5060\n",
      "Epoch 157  Train Loss: 0.4530  Val Loss: 0.5069\n",
      "Epoch 158  Train Loss: 0.4569  Val Loss: 0.5048\n",
      "Epoch 159  Train Loss: 0.4559  Val Loss: 0.5059\n",
      "Epoch 160  Train Loss: 0.4553  Val Loss: 0.5057\n",
      "Epoch 161  Train Loss: 0.4542  Val Loss: 0.5048\n",
      "Epoch 162  Train Loss: 0.4553  Val Loss: 0.5043\n",
      "Epoch 163  Train Loss: 0.4507  Val Loss: 0.5052\n",
      "Epoch 164  Train Loss: 0.4491  Val Loss: 0.5059\n",
      "Epoch 165  Train Loss: 0.4512  Val Loss: 0.5047\n",
      "Epoch 166  Train Loss: 0.4516  Val Loss: 0.5037\n",
      "Epoch 167  Train Loss: 0.4499  Val Loss: 0.5052\n",
      "Epoch 168  Train Loss: 0.4532  Val Loss: 0.5058\n",
      "Epoch 169  Train Loss: 0.4495  Val Loss: 0.5047\n",
      "Epoch 170  Train Loss: 0.4492  Val Loss: 0.5052\n",
      "Epoch 171  Train Loss: 0.4450  Val Loss: 0.5041\n",
      "Epoch 172  Train Loss: 0.4496  Val Loss: 0.5041\n",
      "Epoch 173  Train Loss: 0.4486  Val Loss: 0.5034\n",
      "Epoch 174  Train Loss: 0.4474  Val Loss: 0.5057\n",
      "Epoch 175  Train Loss: 0.4512  Val Loss: 0.5039\n",
      "Epoch 176  Train Loss: 0.4460  Val Loss: 0.5050\n",
      "Epoch 177  Train Loss: 0.4477  Val Loss: 0.5054\n",
      "Epoch 178  Train Loss: 0.4474  Val Loss: 0.5040\n",
      "Epoch 179  Train Loss: 0.4494  Val Loss: 0.5035\n",
      "Epoch 180  Train Loss: 0.4454  Val Loss: 0.5039\n",
      "Epoch 181  Train Loss: 0.4463  Val Loss: 0.5067\n",
      "Epoch 182  Train Loss: 0.4423  Val Loss: 0.5030\n",
      "Epoch 183  Train Loss: 0.4468  Val Loss: 0.5019\n",
      "Epoch 184  Train Loss: 0.4407  Val Loss: 0.5040\n",
      "Epoch 185  Train Loss: 0.4444  Val Loss: 0.5036\n",
      "Epoch 186  Train Loss: 0.4459  Val Loss: 0.5038\n",
      "Epoch 187  Train Loss: 0.4433  Val Loss: 0.5024\n",
      "Epoch 188  Train Loss: 0.4433  Val Loss: 0.5043\n",
      "Epoch 189  Train Loss: 0.4408  Val Loss: 0.5036\n",
      "Epoch 190  Train Loss: 0.4450  Val Loss: 0.5044\n",
      "Epoch 191  Train Loss: 0.4390  Val Loss: 0.5033\n",
      "Epoch 192  Train Loss: 0.4450  Val Loss: 0.5029\n",
      "Epoch 193  Train Loss: 0.4401  Val Loss: 0.5039\n",
      "Epoch 194  Train Loss: 0.4467  Val Loss: 0.5023\n",
      "Epoch 195  Train Loss: 0.4388  Val Loss: 0.5025\n",
      "Epoch 196  Train Loss: 0.4411  Val Loss: 0.5019\n",
      "Epoch 197  Train Loss: 0.4370  Val Loss: 0.5025\n",
      "Epoch 198  Train Loss: 0.4432  Val Loss: 0.5030\n",
      "Epoch 199  Train Loss: 0.4403  Val Loss: 0.5020\n",
      "Epoch 200  Train Loss: 0.4377  Val Loss: 0.5019\n",
      "Epoch 201  Train Loss: 0.4359  Val Loss: 0.5031\n",
      "Epoch 202  Train Loss: 0.4399  Val Loss: 0.5033\n",
      "Epoch 203  Train Loss: 0.4384  Val Loss: 0.5020\n",
      "Epoch 204  Train Loss: 0.4403  Val Loss: 0.5039\n",
      "Epoch 205  Train Loss: 0.4412  Val Loss: 0.5025\n",
      "Epoch 206  Train Loss: 0.4348  Val Loss: 0.5027\n",
      "Epoch 207  Train Loss: 0.4389  Val Loss: 0.5028\n",
      "Epoch 208  Train Loss: 0.4342  Val Loss: 0.5037\n",
      "Epoch 209  Train Loss: 0.4388  Val Loss: 0.5030\n",
      "Epoch 210  Train Loss: 0.4351  Val Loss: 0.5022\n",
      "Epoch 211  Train Loss: 0.4386  Val Loss: 0.5038\n",
      "Epoch 212  Train Loss: 0.4372  Val Loss: 0.5029\n",
      "Epoch 213  Train Loss: 0.4333  Val Loss: 0.5025\n",
      "Epoch 214  Train Loss: 0.4337  Val Loss: 0.5033\n",
      "Epoch 215  Train Loss: 0.4320  Val Loss: 0.5043\n",
      "Epoch 216  Train Loss: 0.4369  Val Loss: 0.5031\n",
      "Epoch 217  Train Loss: 0.4328  Val Loss: 0.5023\n",
      "Epoch 218  Train Loss: 0.4314  Val Loss: 0.5017\n",
      "Epoch 219  Train Loss: 0.4316  Val Loss: 0.5026\n",
      "Epoch 220  Train Loss: 0.4308  Val Loss: 0.5024\n",
      "Epoch 221  Train Loss: 0.4314  Val Loss: 0.5022\n",
      "Epoch 222  Train Loss: 0.4317  Val Loss: 0.5026\n",
      "Epoch 223  Train Loss: 0.4305  Val Loss: 0.5025\n",
      "Epoch 224  Train Loss: 0.4301  Val Loss: 0.5041\n",
      "Epoch 225  Train Loss: 0.4285  Val Loss: 0.5014\n",
      "Epoch 226  Train Loss: 0.4305  Val Loss: 0.5019\n",
      "Epoch 227  Train Loss: 0.4285  Val Loss: 0.5026\n",
      "Epoch 228  Train Loss: 0.4350  Val Loss: 0.5011\n",
      "Epoch 229  Train Loss: 0.4338  Val Loss: 0.5028\n",
      "Epoch 230  Train Loss: 0.4254  Val Loss: 0.5018\n",
      "Epoch 231  Train Loss: 0.4327  Val Loss: 0.5023\n",
      "Epoch 232  Train Loss: 0.4265  Val Loss: 0.5033\n",
      "Epoch 233  Train Loss: 0.4244  Val Loss: 0.5027\n",
      "Epoch 234  Train Loss: 0.4308  Val Loss: 0.5016\n",
      "Epoch 235  Train Loss: 0.4267  Val Loss: 0.5033\n",
      "Epoch 236  Train Loss: 0.4321  Val Loss: 0.5046\n",
      "Epoch 237  Train Loss: 0.4287  Val Loss: 0.5029\n",
      "Epoch 238  Train Loss: 0.4281  Val Loss: 0.5031\n",
      "Epoch 239  Train Loss: 0.4249  Val Loss: 0.5024\n",
      "Epoch 240  Train Loss: 0.4222  Val Loss: 0.5031\n",
      "Epoch 241  Train Loss: 0.4287  Val Loss: 0.5023\n",
      "Epoch 242  Train Loss: 0.4264  Val Loss: 0.5027\n",
      "Epoch 243  Train Loss: 0.4193  Val Loss: 0.5023\n",
      "Epoch 244  Train Loss: 0.4267  Val Loss: 0.5032\n",
      "Epoch 245  Train Loss: 0.4246  Val Loss: 0.5027\n",
      "Epoch 246  Train Loss: 0.4211  Val Loss: 0.5027\n",
      "Epoch 247  Train Loss: 0.4235  Val Loss: 0.5026\n",
      "Epoch 248  Train Loss: 0.4284  Val Loss: 0.5023\n",
      "Epoch 249  Train Loss: 0.4233  Val Loss: 0.5025\n",
      "Epoch 250  Train Loss: 0.4253  Val Loss: 0.5020\n",
      "Epoch 251  Train Loss: 0.4236  Val Loss: 0.5034\n",
      "Epoch 252  Train Loss: 0.4159  Val Loss: 0.5025\n",
      "Epoch 253  Train Loss: 0.4236  Val Loss: 0.5023\n",
      "Epoch 254  Train Loss: 0.4220  Val Loss: 0.5015\n",
      "Epoch 255  Train Loss: 0.4216  Val Loss: 0.5028\n",
      "Epoch 256  Train Loss: 0.4203  Val Loss: 0.5039\n",
      "Epoch 257  Train Loss: 0.4204  Val Loss: 0.5029\n",
      "Epoch 258  Train Loss: 0.4175  Val Loss: 0.5024\n",
      "Epoch 259  Train Loss: 0.4203  Val Loss: 0.5031\n",
      "Epoch 260  Train Loss: 0.4213  Val Loss: 0.5024\n",
      "Epoch 261  Train Loss: 0.4171  Val Loss: 0.5020\n",
      "Epoch 262  Train Loss: 0.4189  Val Loss: 0.5027\n",
      "Epoch 263  Train Loss: 0.4187  Val Loss: 0.5019\n",
      "Epoch 264  Train Loss: 0.4205  Val Loss: 0.5020\n",
      "Epoch 265  Train Loss: 0.4188  Val Loss: 0.5023\n",
      "Epoch 266  Train Loss: 0.4156  Val Loss: 0.5019\n",
      "Epoch 267  Train Loss: 0.4199  Val Loss: 0.5017\n",
      "Epoch 268  Train Loss: 0.4190  Val Loss: 0.5027\n",
      "Epoch 269  Train Loss: 0.4195  Val Loss: 0.5016\n",
      "Epoch 270  Train Loss: 0.4207  Val Loss: 0.5019\n",
      "Epoch 271  Train Loss: 0.4164  Val Loss: 0.5036\n",
      "Epoch 272  Train Loss: 0.4169  Val Loss: 0.5014\n",
      "Epoch 273  Train Loss: 0.4143  Val Loss: 0.5008\n",
      "Epoch 274  Train Loss: 0.4195  Val Loss: 0.5023\n",
      "Epoch 275  Train Loss: 0.4124  Val Loss: 0.5022\n",
      "Epoch 276  Train Loss: 0.4151  Val Loss: 0.5034\n",
      "Epoch 277  Train Loss: 0.4178  Val Loss: 0.5031\n",
      "Epoch 278  Train Loss: 0.4150  Val Loss: 0.5028\n",
      "Epoch 279  Train Loss: 0.4106  Val Loss: 0.5040\n",
      "Epoch 280  Train Loss: 0.4144  Val Loss: 0.5034\n",
      "Epoch 281  Train Loss: 0.4133  Val Loss: 0.5035\n",
      "Epoch 282  Train Loss: 0.4120  Val Loss: 0.5028\n",
      "Epoch 283  Train Loss: 0.4182  Val Loss: 0.5034\n",
      "Epoch 284  Train Loss: 0.4113  Val Loss: 0.5040\n",
      "Epoch 285  Train Loss: 0.4121  Val Loss: 0.5033\n",
      "Epoch 286  Train Loss: 0.4140  Val Loss: 0.5022\n",
      "Epoch 287  Train Loss: 0.4148  Val Loss: 0.5033\n",
      "Epoch 288  Train Loss: 0.4123  Val Loss: 0.5022\n",
      "Epoch 289  Train Loss: 0.4131  Val Loss: 0.5035\n",
      "Epoch 290  Train Loss: 0.4117  Val Loss: 0.5018\n",
      "Epoch 291  Train Loss: 0.4123  Val Loss: 0.5022\n",
      "Epoch 292  Train Loss: 0.4105  Val Loss: 0.5025\n",
      "Epoch 293  Train Loss: 0.4135  Val Loss: 0.5018\n",
      "Epoch 294  Train Loss: 0.4155  Val Loss: 0.5022\n",
      "Epoch 295  Train Loss: 0.4042  Val Loss: 0.5020\n",
      "Epoch 296  Train Loss: 0.4167  Val Loss: 0.5022\n",
      "Epoch 297  Train Loss: 0.4098  Val Loss: 0.5015\n",
      "Epoch 298  Train Loss: 0.4081  Val Loss: 0.5025\n",
      "Epoch 299  Train Loss: 0.4104  Val Loss: 0.5020\n",
      "Epoch 300  Train Loss: 0.4112  Val Loss: 0.5023\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.04      0.07       270\n",
      "         1.0       0.79      0.96      0.87      1010\n",
      "\n",
      "    accuracy                           0.77      1280\n",
      "   macro avg       0.51      0.50      0.47      1280\n",
      "weighted avg       0.67      0.77      0.70      1280\n",
      "\n",
      "Test ROC AUC: 0.5140374037403741\n"
     ]
    }
   ],
   "source": [
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=gemma_train_embs.shape[1]).to(device)\n",
    "\n",
    "# 4) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-7, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# 5) Training & Validation Loop\n",
    "n_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # -- Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # -- Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 6) Load best model and test evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_probs = []\n",
    "y_true  = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        probs = model(xb)\n",
    "        y_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "        y_true .extend(yb.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_pred = (np.array(y_probs) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fabaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMeanDataset(Dataset):\n",
    "    def __init__(self, aligned_embs: List[NDArray[np.float32]], labels):\n",
    "        \n",
    "        self.mean_embs = np.mean(aligned_embs, axis=0)\n",
    "        self.num_models = len(aligned_embs)\n",
    "        self.emb_dim = aligned_embs[0].shape[1]\n",
    "        self.labels = labels\n",
    "\n",
    "        assert self.mean_embs.shape[0] == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mean_embs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.mean_embs[index]\n",
    "        y = self.labels[index] \n",
    "\n",
    "        x_tensor = torch.from_numpy(x)\n",
    "        y_tensor = torch.tensor(y)\n",
    "\n",
    "        return x_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e99c0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.bincount(torch.tensor(y_train.astype(int)))\n",
    "# weight for each class = 1 / count\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "# assign a sample-level weight based on its label\n",
    "sample_weights = class_weights[y_train]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights, \n",
    "    num_samples=len(sample_weights), \n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "40bab6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aligned_embs= [gemma_train_aligned, llama_train_aligned, qwen_train_aligned]\n",
    "val_aligned_embs= [gemma_val_aligned, llama_val_aligned, qwen_val_aligned]\n",
    "test_aligned_embs= [gemma_test_aligned, llama_test_aligned, qwen_test_aligned]\n",
    "\n",
    "train_ds = EmbeddingMeanDataset(train_aligned_embs, y_train)\n",
    "val_ds = EmbeddingMeanDataset(val_aligned_embs, y_val)\n",
    "test_ds = EmbeddingMeanDataset(test_aligned_embs, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fed0b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = gemma_train_embs.shape[1]\n",
    "c = 1\n",
    "\n",
    "model = nn.Linear(input_dim, c)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "04bae7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train Loss: 0.7830  Val Loss: 0.7461\n",
      "Epoch  2  Train Loss: 0.7717  Val Loss: 0.7332\n",
      "Epoch  3  Train Loss: 0.7652  Val Loss: 0.7378\n",
      "Epoch  4  Train Loss: 0.7590  Val Loss: 0.7267\n",
      "Epoch  5  Train Loss: 0.7524  Val Loss: 0.7299\n",
      "Epoch  6  Train Loss: 0.7448  Val Loss: 0.7258\n",
      "Epoch  7  Train Loss: 0.7395  Val Loss: 0.7098\n",
      "Epoch  8  Train Loss: 0.7312  Val Loss: 0.7074\n",
      "Epoch  9  Train Loss: 0.7263  Val Loss: 0.7025\n",
      "Epoch 10  Train Loss: 0.7210  Val Loss: 0.7017\n",
      "Epoch 11  Train Loss: 0.7124  Val Loss: 0.6916\n",
      "Epoch 12  Train Loss: 0.7047  Val Loss: 0.6893\n",
      "Epoch 13  Train Loss: 0.7056  Val Loss: 0.6903\n",
      "Epoch 14  Train Loss: 0.7028  Val Loss: 0.6841\n",
      "Epoch 15  Train Loss: 0.6988  Val Loss: 0.6705\n",
      "Epoch 16  Train Loss: 0.6913  Val Loss: 0.6742\n",
      "Epoch 17  Train Loss: 0.6835  Val Loss: 0.6792\n",
      "Epoch 18  Train Loss: 0.6779  Val Loss: 0.6693\n",
      "Epoch 19  Train Loss: 0.6769  Val Loss: 0.6637\n",
      "Epoch 20  Train Loss: 0.6670  Val Loss: 0.6595\n",
      "Epoch 21  Train Loss: 0.6661  Val Loss: 0.6581\n",
      "Epoch 22  Train Loss: 0.6603  Val Loss: 0.6628\n",
      "Epoch 23  Train Loss: 0.6590  Val Loss: 0.6573\n",
      "Epoch 24  Train Loss: 0.6533  Val Loss: 0.6469\n",
      "Epoch 25  Train Loss: 0.6527  Val Loss: 0.6456\n",
      "Epoch 26  Train Loss: 0.6474  Val Loss: 0.6435\n",
      "Epoch 27  Train Loss: 0.6388  Val Loss: 0.6354\n",
      "Epoch 28  Train Loss: 0.6348  Val Loss: 0.6351\n",
      "Epoch 29  Train Loss: 0.6333  Val Loss: 0.6331\n",
      "Epoch 30  Train Loss: 0.6260  Val Loss: 0.6270\n",
      "Epoch 31  Train Loss: 0.6230  Val Loss: 0.6287\n",
      "Epoch 32  Train Loss: 0.6257  Val Loss: 0.6185\n",
      "Epoch 33  Train Loss: 0.6205  Val Loss: 0.6204\n",
      "Epoch 34  Train Loss: 0.6156  Val Loss: 0.6174\n",
      "Epoch 35  Train Loss: 0.6076  Val Loss: 0.6151\n",
      "Epoch 36  Train Loss: 0.6055  Val Loss: 0.6141\n",
      "Epoch 37  Train Loss: 0.6049  Val Loss: 0.6111\n",
      "Epoch 38  Train Loss: 0.5997  Val Loss: 0.6079\n",
      "Epoch 39  Train Loss: 0.6026  Val Loss: 0.6041\n",
      "Epoch 40  Train Loss: 0.5967  Val Loss: 0.6016\n",
      "Epoch 41  Train Loss: 0.5932  Val Loss: 0.6001\n",
      "Epoch 42  Train Loss: 0.5879  Val Loss: 0.6017\n",
      "Epoch 43  Train Loss: 0.5829  Val Loss: 0.6013\n",
      "Epoch 44  Train Loss: 0.5836  Val Loss: 0.5988\n",
      "Epoch 45  Train Loss: 0.5821  Val Loss: 0.5925\n",
      "Epoch 46  Train Loss: 0.5776  Val Loss: 0.5872\n",
      "Epoch 47  Train Loss: 0.5757  Val Loss: 0.5886\n",
      "Epoch 48  Train Loss: 0.5705  Val Loss: 0.5849\n",
      "Epoch 49  Train Loss: 0.5659  Val Loss: 0.5850\n",
      "Epoch 50  Train Loss: 0.5701  Val Loss: 0.5744\n",
      "Epoch 51  Train Loss: 0.5639  Val Loss: 0.5798\n",
      "Epoch 52  Train Loss: 0.5625  Val Loss: 0.5754\n",
      "Epoch 53  Train Loss: 0.5561  Val Loss: 0.5703\n",
      "Epoch 54  Train Loss: 0.5532  Val Loss: 0.5709\n",
      "Epoch 55  Train Loss: 0.5518  Val Loss: 0.5661\n",
      "Epoch 56  Train Loss: 0.5532  Val Loss: 0.5693\n",
      "Epoch 57  Train Loss: 0.5472  Val Loss: 0.5657\n",
      "Epoch 58  Train Loss: 0.5486  Val Loss: 0.5690\n",
      "Epoch 59  Train Loss: 0.5443  Val Loss: 0.5628\n",
      "Epoch 60  Train Loss: 0.5434  Val Loss: 0.5597\n",
      "Epoch 61  Train Loss: 0.5432  Val Loss: 0.5545\n",
      "Epoch 62  Train Loss: 0.5377  Val Loss: 0.5619\n",
      "Epoch 63  Train Loss: 0.5357  Val Loss: 0.5585\n",
      "Epoch 64  Train Loss: 0.5360  Val Loss: 0.5512\n",
      "Epoch 65  Train Loss: 0.5353  Val Loss: 0.5512\n",
      "Epoch 66  Train Loss: 0.5326  Val Loss: 0.5510\n",
      "Epoch 67  Train Loss: 0.5283  Val Loss: 0.5468\n",
      "Epoch 68  Train Loss: 0.5261  Val Loss: 0.5492\n",
      "Epoch 69  Train Loss: 0.5265  Val Loss: 0.5499\n",
      "Epoch 70  Train Loss: 0.5203  Val Loss: 0.5468\n",
      "Epoch 71  Train Loss: 0.5242  Val Loss: 0.5437\n",
      "Epoch 72  Train Loss: 0.5232  Val Loss: 0.5455\n",
      "Epoch 73  Train Loss: 0.5172  Val Loss: 0.5424\n",
      "Epoch 74  Train Loss: 0.5165  Val Loss: 0.5453\n",
      "Epoch 75  Train Loss: 0.5143  Val Loss: 0.5356\n",
      "Epoch 76  Train Loss: 0.5128  Val Loss: 0.5373\n",
      "Epoch 77  Train Loss: 0.5125  Val Loss: 0.5353\n",
      "Epoch 78  Train Loss: 0.5105  Val Loss: 0.5359\n",
      "Epoch 79  Train Loss: 0.5079  Val Loss: 0.5373\n",
      "Epoch 80  Train Loss: 0.5097  Val Loss: 0.5328\n",
      "Epoch 81  Train Loss: 0.5061  Val Loss: 0.5325\n",
      "Epoch 82  Train Loss: 0.5059  Val Loss: 0.5374\n",
      "Epoch 83  Train Loss: 0.5036  Val Loss: 0.5367\n",
      "Epoch 84  Train Loss: 0.5003  Val Loss: 0.5295\n",
      "Epoch 85  Train Loss: 0.4989  Val Loss: 0.5315\n",
      "Epoch 86  Train Loss: 0.4945  Val Loss: 0.5245\n",
      "Epoch 87  Train Loss: 0.5007  Val Loss: 0.5279\n",
      "Epoch 88  Train Loss: 0.4988  Val Loss: 0.5280\n",
      "Epoch 89  Train Loss: 0.4969  Val Loss: 0.5262\n",
      "Epoch 90  Train Loss: 0.4930  Val Loss: 0.5277\n",
      "Epoch 91  Train Loss: 0.4938  Val Loss: 0.5225\n",
      "Epoch 92  Train Loss: 0.4908  Val Loss: 0.5232\n",
      "Epoch 93  Train Loss: 0.4893  Val Loss: 0.5225\n",
      "Epoch 94  Train Loss: 0.4863  Val Loss: 0.5232\n",
      "Epoch 95  Train Loss: 0.4836  Val Loss: 0.5220\n",
      "Epoch 96  Train Loss: 0.4810  Val Loss: 0.5207\n",
      "Epoch 97  Train Loss: 0.4807  Val Loss: 0.5183\n",
      "Epoch 98  Train Loss: 0.4847  Val Loss: 0.5207\n",
      "Epoch 99  Train Loss: 0.4837  Val Loss: 0.5215\n",
      "Epoch 100  Train Loss: 0.4866  Val Loss: 0.5172\n",
      "Epoch 101  Train Loss: 0.4820  Val Loss: 0.5176\n",
      "Epoch 102  Train Loss: 0.4785  Val Loss: 0.5172\n",
      "Epoch 103  Train Loss: 0.4786  Val Loss: 0.5167\n",
      "Epoch 104  Train Loss: 0.4794  Val Loss: 0.5168\n",
      "Epoch 105  Train Loss: 0.4787  Val Loss: 0.5159\n",
      "Epoch 106  Train Loss: 0.4771  Val Loss: 0.5135\n",
      "Epoch 107  Train Loss: 0.4753  Val Loss: 0.5126\n",
      "Epoch 108  Train Loss: 0.4752  Val Loss: 0.5154\n",
      "Epoch 109  Train Loss: 0.4724  Val Loss: 0.5108\n",
      "Epoch 110  Train Loss: 0.4752  Val Loss: 0.5142\n",
      "Epoch 111  Train Loss: 0.4722  Val Loss: 0.5139\n",
      "Epoch 112  Train Loss: 0.4707  Val Loss: 0.5120\n",
      "Epoch 113  Train Loss: 0.4696  Val Loss: 0.5119\n",
      "Epoch 114  Train Loss: 0.4648  Val Loss: 0.5113\n",
      "Epoch 115  Train Loss: 0.4677  Val Loss: 0.5122\n",
      "Epoch 116  Train Loss: 0.4653  Val Loss: 0.5083\n",
      "Epoch 117  Train Loss: 0.4680  Val Loss: 0.5092\n",
      "Epoch 118  Train Loss: 0.4602  Val Loss: 0.5089\n",
      "Epoch 119  Train Loss: 0.4665  Val Loss: 0.5092\n",
      "Epoch 120  Train Loss: 0.4638  Val Loss: 0.5093\n",
      "Epoch 121  Train Loss: 0.4616  Val Loss: 0.5075\n",
      "Epoch 122  Train Loss: 0.4626  Val Loss: 0.5091\n",
      "Epoch 123  Train Loss: 0.4636  Val Loss: 0.5089\n",
      "Epoch 124  Train Loss: 0.4595  Val Loss: 0.5060\n",
      "Epoch 125  Train Loss: 0.4610  Val Loss: 0.5062\n",
      "Epoch 126  Train Loss: 0.4637  Val Loss: 0.5074\n",
      "Epoch 127  Train Loss: 0.4630  Val Loss: 0.5061\n",
      "Epoch 128  Train Loss: 0.4607  Val Loss: 0.5086\n",
      "Epoch 129  Train Loss: 0.4587  Val Loss: 0.5044\n",
      "Epoch 130  Train Loss: 0.4573  Val Loss: 0.5062\n",
      "Epoch 131  Train Loss: 0.4557  Val Loss: 0.5055\n",
      "Epoch 132  Train Loss: 0.4563  Val Loss: 0.5099\n",
      "Epoch 133  Train Loss: 0.4536  Val Loss: 0.5045\n",
      "Epoch 134  Train Loss: 0.4545  Val Loss: 0.5025\n",
      "Epoch 135  Train Loss: 0.4555  Val Loss: 0.5043\n",
      "Epoch 136  Train Loss: 0.4506  Val Loss: 0.5028\n",
      "Epoch 137  Train Loss: 0.4542  Val Loss: 0.5036\n",
      "Epoch 138  Train Loss: 0.4518  Val Loss: 0.5037\n",
      "Epoch 139  Train Loss: 0.4548  Val Loss: 0.5013\n",
      "Epoch 140  Train Loss: 0.4515  Val Loss: 0.5018\n",
      "Epoch 141  Train Loss: 0.4477  Val Loss: 0.5020\n",
      "Epoch 142  Train Loss: 0.4467  Val Loss: 0.5048\n",
      "Epoch 143  Train Loss: 0.4504  Val Loss: 0.5019\n",
      "Epoch 144  Train Loss: 0.4506  Val Loss: 0.5017\n",
      "Epoch 145  Train Loss: 0.4487  Val Loss: 0.5011\n",
      "Epoch 146  Train Loss: 0.4511  Val Loss: 0.5025\n",
      "Epoch 147  Train Loss: 0.4483  Val Loss: 0.4997\n",
      "Epoch 148  Train Loss: 0.4446  Val Loss: 0.5022\n",
      "Epoch 149  Train Loss: 0.4424  Val Loss: 0.5034\n",
      "Epoch 150  Train Loss: 0.4438  Val Loss: 0.5014\n",
      "Epoch 151  Train Loss: 0.4428  Val Loss: 0.5009\n",
      "Epoch 152  Train Loss: 0.4405  Val Loss: 0.4999\n",
      "Epoch 153  Train Loss: 0.4442  Val Loss: 0.4982\n",
      "Epoch 154  Train Loss: 0.4403  Val Loss: 0.5018\n",
      "Epoch 155  Train Loss: 0.4433  Val Loss: 0.5001\n",
      "Epoch 156  Train Loss: 0.4362  Val Loss: 0.4998\n",
      "Epoch 157  Train Loss: 0.4378  Val Loss: 0.4985\n",
      "Epoch 158  Train Loss: 0.4400  Val Loss: 0.4998\n",
      "Epoch 159  Train Loss: 0.4382  Val Loss: 0.4984\n",
      "Epoch 160  Train Loss: 0.4400  Val Loss: 0.5004\n",
      "Epoch 161  Train Loss: 0.4415  Val Loss: 0.4994\n",
      "Epoch 162  Train Loss: 0.4383  Val Loss: 0.4989\n",
      "Epoch 163  Train Loss: 0.4353  Val Loss: 0.4996\n",
      "Epoch 164  Train Loss: 0.4380  Val Loss: 0.4982\n",
      "Epoch 165  Train Loss: 0.4305  Val Loss: 0.4994\n",
      "Epoch 166  Train Loss: 0.4364  Val Loss: 0.4981\n",
      "Epoch 167  Train Loss: 0.4367  Val Loss: 0.4987\n",
      "Epoch 168  Train Loss: 0.4344  Val Loss: 0.4969\n",
      "Epoch 169  Train Loss: 0.4347  Val Loss: 0.4972\n",
      "Epoch 170  Train Loss: 0.4387  Val Loss: 0.4956\n",
      "Epoch 171  Train Loss: 0.4350  Val Loss: 0.4987\n",
      "Epoch 172  Train Loss: 0.4378  Val Loss: 0.4989\n",
      "Epoch 173  Train Loss: 0.4313  Val Loss: 0.4977\n",
      "Epoch 174  Train Loss: 0.4330  Val Loss: 0.4976\n",
      "Epoch 175  Train Loss: 0.4293  Val Loss: 0.4981\n",
      "Epoch 176  Train Loss: 0.4306  Val Loss: 0.4995\n",
      "Epoch 177  Train Loss: 0.4313  Val Loss: 0.4971\n",
      "Epoch 178  Train Loss: 0.4317  Val Loss: 0.4982\n",
      "Epoch 179  Train Loss: 0.4327  Val Loss: 0.4982\n",
      "Epoch 180  Train Loss: 0.4301  Val Loss: 0.5003\n",
      "Epoch 181  Train Loss: 0.4272  Val Loss: 0.4987\n",
      "Epoch 182  Train Loss: 0.4275  Val Loss: 0.4956\n",
      "Epoch 183  Train Loss: 0.4270  Val Loss: 0.4985\n",
      "Epoch 184  Train Loss: 0.4276  Val Loss: 0.4983\n",
      "Epoch 185  Train Loss: 0.4303  Val Loss: 0.4990\n",
      "Epoch 186  Train Loss: 0.4226  Val Loss: 0.4967\n",
      "Epoch 187  Train Loss: 0.4252  Val Loss: 0.4983\n",
      "Epoch 188  Train Loss: 0.4274  Val Loss: 0.4971\n",
      "Epoch 189  Train Loss: 0.4260  Val Loss: 0.4984\n",
      "Epoch 190  Train Loss: 0.4227  Val Loss: 0.4986\n",
      "Epoch 191  Train Loss: 0.4259  Val Loss: 0.4983\n",
      "Epoch 192  Train Loss: 0.4236  Val Loss: 0.4974\n",
      "Epoch 193  Train Loss: 0.4285  Val Loss: 0.4963\n",
      "Epoch 194  Train Loss: 0.4239  Val Loss: 0.4965\n",
      "Epoch 195  Train Loss: 0.4241  Val Loss: 0.4975\n",
      "Epoch 196  Train Loss: 0.4226  Val Loss: 0.4978\n",
      "Epoch 197  Train Loss: 0.4256  Val Loss: 0.4958\n",
      "Epoch 198  Train Loss: 0.4254  Val Loss: 0.4944\n",
      "Epoch 199  Train Loss: 0.4139  Val Loss: 0.4974\n",
      "Epoch 200  Train Loss: 0.4223  Val Loss: 0.4970\n",
      "Epoch 201  Train Loss: 0.4234  Val Loss: 0.4966\n",
      "Epoch 202  Train Loss: 0.4169  Val Loss: 0.4971\n",
      "Epoch 203  Train Loss: 0.4196  Val Loss: 0.4962\n",
      "Epoch 204  Train Loss: 0.4194  Val Loss: 0.4991\n",
      "Epoch 205  Train Loss: 0.4210  Val Loss: 0.4964\n",
      "Epoch 206  Train Loss: 0.4169  Val Loss: 0.4958\n",
      "Epoch 207  Train Loss: 0.4187  Val Loss: 0.4985\n",
      "Epoch 208  Train Loss: 0.4165  Val Loss: 0.4957\n",
      "Epoch 209  Train Loss: 0.4193  Val Loss: 0.4975\n",
      "Epoch 210  Train Loss: 0.4151  Val Loss: 0.4968\n",
      "Epoch 211  Train Loss: 0.4163  Val Loss: 0.4966\n",
      "Epoch 212  Train Loss: 0.4159  Val Loss: 0.4975\n",
      "Epoch 213  Train Loss: 0.4157  Val Loss: 0.4958\n",
      "Epoch 214  Train Loss: 0.4170  Val Loss: 0.4962\n",
      "Epoch 215  Train Loss: 0.4223  Val Loss: 0.4955\n",
      "Epoch 216  Train Loss: 0.4149  Val Loss: 0.4970\n",
      "Epoch 217  Train Loss: 0.4145  Val Loss: 0.4971\n",
      "Epoch 218  Train Loss: 0.4131  Val Loss: 0.4969\n",
      "Epoch 219  Train Loss: 0.4088  Val Loss: 0.4971\n",
      "Epoch 220  Train Loss: 0.4100  Val Loss: 0.4972\n",
      "Epoch 221  Train Loss: 0.4145  Val Loss: 0.4972\n",
      "Epoch 222  Train Loss: 0.4134  Val Loss: 0.4981\n",
      "Epoch 223  Train Loss: 0.4142  Val Loss: 0.4965\n",
      "Epoch 224  Train Loss: 0.4088  Val Loss: 0.4957\n",
      "Epoch 225  Train Loss: 0.4127  Val Loss: 0.4978\n",
      "Epoch 226  Train Loss: 0.4092  Val Loss: 0.4970\n",
      "Epoch 227  Train Loss: 0.4166  Val Loss: 0.4972\n",
      "Epoch 228  Train Loss: 0.4084  Val Loss: 0.4969\n",
      "Epoch 229  Train Loss: 0.4075  Val Loss: 0.4973\n",
      "Epoch 230  Train Loss: 0.4115  Val Loss: 0.4986\n",
      "Epoch 231  Train Loss: 0.4111  Val Loss: 0.4981\n",
      "Epoch 232  Train Loss: 0.4075  Val Loss: 0.4982\n",
      "Epoch 233  Train Loss: 0.4072  Val Loss: 0.4967\n",
      "Epoch 234  Train Loss: 0.4048  Val Loss: 0.4982\n",
      "Epoch 235  Train Loss: 0.4095  Val Loss: 0.4981\n",
      "Epoch 236  Train Loss: 0.4094  Val Loss: 0.4983\n",
      "Epoch 237  Train Loss: 0.4094  Val Loss: 0.4965\n",
      "Epoch 238  Train Loss: 0.4072  Val Loss: 0.4971\n",
      "Epoch 239  Train Loss: 0.4047  Val Loss: 0.4984\n",
      "Epoch 240  Train Loss: 0.4080  Val Loss: 0.4964\n",
      "Epoch 241  Train Loss: 0.4079  Val Loss: 0.4992\n",
      "Epoch 242  Train Loss: 0.4036  Val Loss: 0.4976\n",
      "Epoch 243  Train Loss: 0.4033  Val Loss: 0.4973\n",
      "Epoch 244  Train Loss: 0.4034  Val Loss: 0.4992\n",
      "Epoch 245  Train Loss: 0.4039  Val Loss: 0.4968\n",
      "Epoch 246  Train Loss: 0.3972  Val Loss: 0.4967\n",
      "Epoch 247  Train Loss: 0.4013  Val Loss: 0.4971\n",
      "Epoch 248  Train Loss: 0.3992  Val Loss: 0.4971\n",
      "Epoch 249  Train Loss: 0.4059  Val Loss: 0.4967\n",
      "Epoch 250  Train Loss: 0.4023  Val Loss: 0.4991\n",
      "Epoch 251  Train Loss: 0.3983  Val Loss: 0.4965\n",
      "Epoch 252  Train Loss: 0.4023  Val Loss: 0.4976\n",
      "Epoch 253  Train Loss: 0.3994  Val Loss: 0.4979\n",
      "Epoch 254  Train Loss: 0.3982  Val Loss: 0.4992\n",
      "Epoch 255  Train Loss: 0.3981  Val Loss: 0.4986\n",
      "Epoch 256  Train Loss: 0.4012  Val Loss: 0.4987\n",
      "Epoch 257  Train Loss: 0.3990  Val Loss: 0.4982\n",
      "Epoch 258  Train Loss: 0.4042  Val Loss: 0.4972\n",
      "Epoch 259  Train Loss: 0.3990  Val Loss: 0.4994\n",
      "Epoch 260  Train Loss: 0.3995  Val Loss: 0.4968\n",
      "Epoch 261  Train Loss: 0.3983  Val Loss: 0.4977\n",
      "Epoch 262  Train Loss: 0.3981  Val Loss: 0.4989\n",
      "Epoch 263  Train Loss: 0.4001  Val Loss: 0.4987\n",
      "Epoch 264  Train Loss: 0.3889  Val Loss: 0.4977\n",
      "Epoch 265  Train Loss: 0.3998  Val Loss: 0.4997\n",
      "Epoch 266  Train Loss: 0.3972  Val Loss: 0.4996\n",
      "Epoch 267  Train Loss: 0.3973  Val Loss: 0.4984\n",
      "Epoch 268  Train Loss: 0.3964  Val Loss: 0.4983\n",
      "Epoch 269  Train Loss: 0.4007  Val Loss: 0.4981\n",
      "Epoch 270  Train Loss: 0.3910  Val Loss: 0.4989\n",
      "Epoch 271  Train Loss: 0.3946  Val Loss: 0.4994\n",
      "Epoch 272  Train Loss: 0.3909  Val Loss: 0.4981\n",
      "Epoch 273  Train Loss: 0.3976  Val Loss: 0.4972\n",
      "Epoch 274  Train Loss: 0.3913  Val Loss: 0.4984\n",
      "Epoch 275  Train Loss: 0.3945  Val Loss: 0.4982\n",
      "Epoch 276  Train Loss: 0.3916  Val Loss: 0.5007\n",
      "Epoch 277  Train Loss: 0.3892  Val Loss: 0.4984\n",
      "Epoch 278  Train Loss: 0.3888  Val Loss: 0.4989\n",
      "Epoch 279  Train Loss: 0.3928  Val Loss: 0.4991\n",
      "Epoch 280  Train Loss: 0.3935  Val Loss: 0.4983\n",
      "Epoch 281  Train Loss: 0.3888  Val Loss: 0.4974\n",
      "Epoch 282  Train Loss: 0.3892  Val Loss: 0.4988\n",
      "Epoch 283  Train Loss: 0.3899  Val Loss: 0.4994\n",
      "Epoch 284  Train Loss: 0.3890  Val Loss: 0.4990\n",
      "Epoch 285  Train Loss: 0.3923  Val Loss: 0.4981\n",
      "Epoch 286  Train Loss: 0.3844  Val Loss: 0.4980\n",
      "Epoch 287  Train Loss: 0.3887  Val Loss: 0.4994\n",
      "Epoch 288  Train Loss: 0.3880  Val Loss: 0.4976\n",
      "Epoch 289  Train Loss: 0.3891  Val Loss: 0.4981\n",
      "Epoch 290  Train Loss: 0.3908  Val Loss: 0.4991\n",
      "Epoch 291  Train Loss: 0.3892  Val Loss: 0.4992\n",
      "Epoch 292  Train Loss: 0.3833  Val Loss: 0.5001\n",
      "Epoch 293  Train Loss: 0.3872  Val Loss: 0.4987\n",
      "Epoch 294  Train Loss: 0.3893  Val Loss: 0.4983\n",
      "Epoch 295  Train Loss: 0.3873  Val Loss: 0.4986\n",
      "Epoch 296  Train Loss: 0.3877  Val Loss: 0.4989\n",
      "Epoch 297  Train Loss: 0.3849  Val Loss: 0.4992\n",
      "Epoch 298  Train Loss: 0.3851  Val Loss: 0.4985\n",
      "Epoch 299  Train Loss: 0.3827  Val Loss: 0.5001\n",
      "Epoch 300  Train Loss: 0.3828  Val Loss: 0.4995\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.04      0.07       270\n",
      "         1.0       0.79      0.96      0.87      1010\n",
      "\n",
      "    accuracy                           0.77      1280\n",
      "   macro avg       0.50      0.50      0.47      1280\n",
      "weighted avg       0.67      0.77      0.70      1280\n",
      "\n",
      "Test ROC AUC: 0.5125522552255226\n"
     ]
    }
   ],
   "source": [
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=gemma_train_embs.shape[1]).to(device)\n",
    "\n",
    "# 4) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# 5) Training & Validation Loop\n",
    "n_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # -- Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device).float()\n",
    "        yb = yb.to(device).unsqueeze(-1).float()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # -- Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device).float()\n",
    "            yb = yb.to(device).unsqueeze(-1).float()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 6) Load best model and test evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_probs = []\n",
    "y_true  = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device).float()\n",
    "        yb = yb.to(device).unsqueeze(-1).float()\n",
    "        probs = model(xb)\n",
    "        y_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "        y_true .extend(yb.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_pred = (np.array(y_probs) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2c214ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train Loss: 0.6456  Val Loss: 0.6377\n",
      "Epoch  2  Train Loss: 0.6335  Val Loss: 0.6286\n",
      "Epoch  3  Train Loss: 0.6276  Val Loss: 0.6217\n",
      "Epoch  4  Train Loss: 0.6132  Val Loss: 0.6171\n",
      "Epoch  5  Train Loss: 0.6146  Val Loss: 0.6142\n",
      "Epoch  6  Train Loss: 0.6108  Val Loss: 0.6070\n",
      "Epoch  7  Train Loss: 0.6119  Val Loss: 0.5987\n",
      "Epoch  8  Train Loss: 0.6029  Val Loss: 0.6028\n",
      "Epoch  9  Train Loss: 0.6007  Val Loss: 0.5980\n",
      "Epoch 10  Train Loss: 0.5957  Val Loss: 0.5931\n",
      "Epoch 11  Train Loss: 0.5960  Val Loss: 0.5893\n",
      "Epoch 12  Train Loss: 0.5909  Val Loss: 0.5835\n",
      "Epoch 13  Train Loss: 0.5873  Val Loss: 0.5849\n",
      "Epoch 14  Train Loss: 0.5844  Val Loss: 0.5809\n",
      "Epoch 15  Train Loss: 0.5810  Val Loss: 0.5735\n",
      "Epoch 16  Train Loss: 0.5787  Val Loss: 0.5769\n",
      "Epoch 17  Train Loss: 0.5746  Val Loss: 0.5772\n",
      "Epoch 18  Train Loss: 0.5786  Val Loss: 0.5733\n",
      "Epoch 19  Train Loss: 0.5684  Val Loss: 0.5702\n",
      "Epoch 20  Train Loss: 0.5616  Val Loss: 0.5739\n",
      "Epoch 21  Train Loss: 0.5674  Val Loss: 0.5709\n",
      "Epoch 22  Train Loss: 0.5632  Val Loss: 0.5643\n",
      "Epoch 23  Train Loss: 0.5604  Val Loss: 0.5665\n",
      "Epoch 24  Train Loss: 0.5580  Val Loss: 0.5664\n",
      "Epoch 25  Train Loss: 0.5525  Val Loss: 0.5623\n",
      "Epoch 26  Train Loss: 0.5569  Val Loss: 0.5593\n",
      "Epoch 27  Train Loss: 0.5459  Val Loss: 0.5554\n",
      "Epoch 28  Train Loss: 0.5509  Val Loss: 0.5578\n",
      "Epoch 29  Train Loss: 0.5459  Val Loss: 0.5568\n",
      "Epoch 30  Train Loss: 0.5425  Val Loss: 0.5507\n",
      "Epoch 31  Train Loss: 0.5447  Val Loss: 0.5526\n",
      "Epoch 32  Train Loss: 0.5387  Val Loss: 0.5501\n",
      "Epoch 33  Train Loss: 0.5444  Val Loss: 0.5468\n",
      "Epoch 34  Train Loss: 0.5390  Val Loss: 0.5448\n",
      "Epoch 35  Train Loss: 0.5358  Val Loss: 0.5474\n",
      "Epoch 36  Train Loss: 0.5249  Val Loss: 0.5470\n",
      "Epoch 37  Train Loss: 0.5339  Val Loss: 0.5423\n",
      "Epoch 38  Train Loss: 0.5273  Val Loss: 0.5419\n",
      "Epoch 39  Train Loss: 0.5256  Val Loss: 0.5398\n",
      "Epoch 40  Train Loss: 0.5240  Val Loss: 0.5350\n",
      "Epoch 41  Train Loss: 0.5242  Val Loss: 0.5365\n",
      "Epoch 42  Train Loss: 0.5244  Val Loss: 0.5377\n",
      "Epoch 43  Train Loss: 0.5211  Val Loss: 0.5359\n",
      "Epoch 44  Train Loss: 0.5228  Val Loss: 0.5386\n",
      "Epoch 45  Train Loss: 0.5199  Val Loss: 0.5346\n",
      "Epoch 46  Train Loss: 0.5206  Val Loss: 0.5338\n",
      "Epoch 47  Train Loss: 0.5207  Val Loss: 0.5339\n",
      "Epoch 48  Train Loss: 0.5114  Val Loss: 0.5296\n",
      "Epoch 49  Train Loss: 0.5137  Val Loss: 0.5346\n",
      "Epoch 50  Train Loss: 0.5145  Val Loss: 0.5285\n",
      "Epoch 51  Train Loss: 0.5080  Val Loss: 0.5268\n",
      "Epoch 52  Train Loss: 0.5096  Val Loss: 0.5293\n",
      "Epoch 53  Train Loss: 0.5045  Val Loss: 0.5261\n",
      "Epoch 54  Train Loss: 0.5096  Val Loss: 0.5285\n",
      "Epoch 55  Train Loss: 0.5096  Val Loss: 0.5261\n",
      "Epoch 56  Train Loss: 0.5032  Val Loss: 0.5271\n",
      "Epoch 57  Train Loss: 0.5061  Val Loss: 0.5256\n",
      "Epoch 58  Train Loss: 0.4980  Val Loss: 0.5229\n",
      "Epoch 59  Train Loss: 0.4989  Val Loss: 0.5215\n",
      "Epoch 60  Train Loss: 0.4991  Val Loss: 0.5220\n",
      "Epoch 61  Train Loss: 0.4991  Val Loss: 0.5189\n",
      "Epoch 62  Train Loss: 0.4962  Val Loss: 0.5196\n",
      "Epoch 63  Train Loss: 0.4944  Val Loss: 0.5214\n",
      "Epoch 64  Train Loss: 0.4937  Val Loss: 0.5212\n",
      "Epoch 65  Train Loss: 0.4958  Val Loss: 0.5182\n",
      "Epoch 66  Train Loss: 0.4929  Val Loss: 0.5184\n",
      "Epoch 67  Train Loss: 0.4917  Val Loss: 0.5203\n",
      "Epoch 68  Train Loss: 0.4897  Val Loss: 0.5169\n",
      "Epoch 69  Train Loss: 0.4902  Val Loss: 0.5141\n",
      "Epoch 70  Train Loss: 0.4914  Val Loss: 0.5181\n",
      "Epoch 71  Train Loss: 0.4865  Val Loss: 0.5158\n",
      "Epoch 72  Train Loss: 0.4858  Val Loss: 0.5173\n",
      "Epoch 73  Train Loss: 0.4807  Val Loss: 0.5139\n",
      "Epoch 74  Train Loss: 0.4854  Val Loss: 0.5116\n",
      "Epoch 75  Train Loss: 0.4821  Val Loss: 0.5111\n",
      "Epoch 76  Train Loss: 0.4820  Val Loss: 0.5115\n",
      "Epoch 77  Train Loss: 0.4824  Val Loss: 0.5129\n",
      "Epoch 78  Train Loss: 0.4806  Val Loss: 0.5155\n",
      "Epoch 79  Train Loss: 0.4767  Val Loss: 0.5106\n",
      "Epoch 80  Train Loss: 0.4818  Val Loss: 0.5105\n",
      "Epoch 81  Train Loss: 0.4767  Val Loss: 0.5150\n",
      "Epoch 82  Train Loss: 0.4776  Val Loss: 0.5099\n",
      "Epoch 83  Train Loss: 0.4775  Val Loss: 0.5092\n",
      "Epoch 84  Train Loss: 0.4774  Val Loss: 0.5097\n",
      "Epoch 85  Train Loss: 0.4760  Val Loss: 0.5096\n",
      "Epoch 86  Train Loss: 0.4736  Val Loss: 0.5100\n",
      "Epoch 87  Train Loss: 0.4709  Val Loss: 0.5122\n",
      "Epoch 88  Train Loss: 0.4707  Val Loss: 0.5069\n",
      "Epoch 89  Train Loss: 0.4713  Val Loss: 0.5073\n",
      "Epoch 90  Train Loss: 0.4669  Val Loss: 0.5089\n",
      "Epoch 91  Train Loss: 0.4727  Val Loss: 0.5085\n",
      "Epoch 92  Train Loss: 0.4660  Val Loss: 0.5085\n",
      "Epoch 93  Train Loss: 0.4660  Val Loss: 0.5071\n",
      "Epoch 94  Train Loss: 0.4701  Val Loss: 0.5050\n",
      "Epoch 95  Train Loss: 0.4677  Val Loss: 0.5053\n",
      "Epoch 96  Train Loss: 0.4656  Val Loss: 0.5065\n",
      "Epoch 97  Train Loss: 0.4642  Val Loss: 0.5070\n",
      "Epoch 98  Train Loss: 0.4642  Val Loss: 0.5059\n",
      "Epoch 99  Train Loss: 0.4647  Val Loss: 0.5051\n",
      "Epoch 100  Train Loss: 0.4620  Val Loss: 0.5052\n",
      "Epoch 101  Train Loss: 0.4588  Val Loss: 0.5058\n",
      "Epoch 102  Train Loss: 0.4615  Val Loss: 0.5026\n",
      "Epoch 103  Train Loss: 0.4610  Val Loss: 0.5046\n",
      "Epoch 104  Train Loss: 0.4608  Val Loss: 0.5032\n",
      "Epoch 105  Train Loss: 0.4583  Val Loss: 0.5045\n",
      "Epoch 106  Train Loss: 0.4596  Val Loss: 0.5059\n",
      "Epoch 107  Train Loss: 0.4583  Val Loss: 0.5033\n",
      "Epoch 108  Train Loss: 0.4569  Val Loss: 0.5028\n",
      "Epoch 109  Train Loss: 0.4571  Val Loss: 0.5022\n",
      "Epoch 110  Train Loss: 0.4535  Val Loss: 0.5015\n",
      "Epoch 111  Train Loss: 0.4563  Val Loss: 0.5024\n",
      "Epoch 112  Train Loss: 0.4531  Val Loss: 0.5014\n",
      "Epoch 113  Train Loss: 0.4557  Val Loss: 0.5014\n",
      "Epoch 114  Train Loss: 0.4541  Val Loss: 0.5023\n",
      "Epoch 115  Train Loss: 0.4514  Val Loss: 0.5036\n",
      "Epoch 116  Train Loss: 0.4517  Val Loss: 0.5024\n",
      "Epoch 117  Train Loss: 0.4522  Val Loss: 0.5023\n",
      "Epoch 118  Train Loss: 0.4491  Val Loss: 0.5003\n",
      "Epoch 119  Train Loss: 0.4507  Val Loss: 0.5016\n",
      "Epoch 120  Train Loss: 0.4522  Val Loss: 0.5014\n",
      "Epoch 121  Train Loss: 0.4487  Val Loss: 0.5014\n",
      "Epoch 122  Train Loss: 0.4478  Val Loss: 0.5006\n",
      "Epoch 123  Train Loss: 0.4487  Val Loss: 0.5022\n",
      "Epoch 124  Train Loss: 0.4515  Val Loss: 0.5024\n",
      "Epoch 125  Train Loss: 0.4468  Val Loss: 0.5011\n",
      "Epoch 126  Train Loss: 0.4479  Val Loss: 0.5004\n",
      "Epoch 127  Train Loss: 0.4464  Val Loss: 0.5000\n",
      "Epoch 128  Train Loss: 0.4441  Val Loss: 0.5001\n",
      "Epoch 129  Train Loss: 0.4445  Val Loss: 0.4990\n",
      "Epoch 130  Train Loss: 0.4476  Val Loss: 0.5000\n",
      "Epoch 131  Train Loss: 0.4455  Val Loss: 0.4994\n",
      "Epoch 132  Train Loss: 0.4444  Val Loss: 0.4998\n",
      "Epoch 133  Train Loss: 0.4459  Val Loss: 0.4997\n",
      "Epoch 134  Train Loss: 0.4412  Val Loss: 0.4985\n",
      "Epoch 135  Train Loss: 0.4402  Val Loss: 0.4998\n",
      "Epoch 136  Train Loss: 0.4422  Val Loss: 0.4988\n",
      "Epoch 137  Train Loss: 0.4419  Val Loss: 0.4993\n",
      "Epoch 138  Train Loss: 0.4431  Val Loss: 0.4994\n",
      "Epoch 139  Train Loss: 0.4390  Val Loss: 0.4995\n",
      "Epoch 140  Train Loss: 0.4383  Val Loss: 0.4991\n",
      "Epoch 141  Train Loss: 0.4309  Val Loss: 0.4987\n",
      "Epoch 142  Train Loss: 0.4376  Val Loss: 0.4982\n",
      "Epoch 143  Train Loss: 0.4363  Val Loss: 0.4992\n",
      "Epoch 144  Train Loss: 0.4385  Val Loss: 0.4990\n",
      "Epoch 145  Train Loss: 0.4353  Val Loss: 0.4996\n",
      "Epoch 146  Train Loss: 0.4344  Val Loss: 0.4992\n",
      "Epoch 147  Train Loss: 0.4356  Val Loss: 0.4999\n",
      "Epoch 148  Train Loss: 0.4365  Val Loss: 0.4998\n",
      "Epoch 149  Train Loss: 0.4355  Val Loss: 0.5002\n",
      "Epoch 150  Train Loss: 0.4318  Val Loss: 0.4991\n",
      "Epoch 151  Train Loss: 0.4349  Val Loss: 0.4995\n",
      "Epoch 152  Train Loss: 0.4347  Val Loss: 0.4995\n",
      "Epoch 153  Train Loss: 0.4312  Val Loss: 0.4988\n",
      "Epoch 154  Train Loss: 0.4311  Val Loss: 0.4989\n",
      "Epoch 155  Train Loss: 0.4288  Val Loss: 0.4982\n",
      "Epoch 156  Train Loss: 0.4316  Val Loss: 0.4994\n",
      "Epoch 157  Train Loss: 0.4336  Val Loss: 0.4994\n",
      "Epoch 158  Train Loss: 0.4306  Val Loss: 0.4986\n",
      "Epoch 159  Train Loss: 0.4276  Val Loss: 0.4988\n",
      "Epoch 160  Train Loss: 0.4307  Val Loss: 0.4986\n",
      "Epoch 161  Train Loss: 0.4279  Val Loss: 0.4971\n",
      "Epoch 162  Train Loss: 0.4280  Val Loss: 0.4997\n",
      "Epoch 163  Train Loss: 0.4258  Val Loss: 0.5000\n",
      "Epoch 164  Train Loss: 0.4257  Val Loss: 0.4990\n",
      "Epoch 165  Train Loss: 0.4286  Val Loss: 0.4984\n",
      "Epoch 166  Train Loss: 0.4269  Val Loss: 0.4979\n",
      "Epoch 167  Train Loss: 0.4283  Val Loss: 0.4970\n",
      "Epoch 168  Train Loss: 0.4237  Val Loss: 0.4978\n",
      "Epoch 169  Train Loss: 0.4258  Val Loss: 0.4974\n",
      "Epoch 170  Train Loss: 0.4254  Val Loss: 0.4992\n",
      "Epoch 171  Train Loss: 0.4217  Val Loss: 0.4986\n",
      "Epoch 172  Train Loss: 0.4273  Val Loss: 0.4985\n",
      "Epoch 173  Train Loss: 0.4201  Val Loss: 0.4984\n",
      "Epoch 174  Train Loss: 0.4221  Val Loss: 0.4978\n",
      "Epoch 175  Train Loss: 0.4185  Val Loss: 0.4978\n",
      "Epoch 176  Train Loss: 0.4237  Val Loss: 0.4993\n",
      "Epoch 177  Train Loss: 0.4226  Val Loss: 0.4985\n",
      "Epoch 178  Train Loss: 0.4131  Val Loss: 0.4979\n",
      "Epoch 179  Train Loss: 0.4247  Val Loss: 0.4973\n",
      "Epoch 180  Train Loss: 0.4200  Val Loss: 0.4980\n",
      "Epoch 181  Train Loss: 0.4156  Val Loss: 0.4993\n",
      "Epoch 182  Train Loss: 0.4196  Val Loss: 0.4965\n",
      "Epoch 183  Train Loss: 0.4229  Val Loss: 0.4975\n",
      "Epoch 184  Train Loss: 0.4165  Val Loss: 0.4971\n",
      "Epoch 185  Train Loss: 0.4213  Val Loss: 0.4990\n",
      "Epoch 186  Train Loss: 0.4152  Val Loss: 0.4997\n",
      "Epoch 187  Train Loss: 0.4196  Val Loss: 0.4999\n",
      "Epoch 188  Train Loss: 0.4181  Val Loss: 0.4987\n",
      "Epoch 189  Train Loss: 0.4132  Val Loss: 0.4990\n",
      "Epoch 190  Train Loss: 0.4150  Val Loss: 0.4988\n",
      "Epoch 191  Train Loss: 0.4152  Val Loss: 0.4983\n",
      "Epoch 192  Train Loss: 0.4137  Val Loss: 0.4975\n",
      "Epoch 193  Train Loss: 0.4135  Val Loss: 0.4993\n",
      "Epoch 194  Train Loss: 0.4131  Val Loss: 0.4982\n",
      "Epoch 195  Train Loss: 0.4117  Val Loss: 0.4977\n",
      "Epoch 196  Train Loss: 0.4146  Val Loss: 0.4978\n",
      "Epoch 197  Train Loss: 0.4157  Val Loss: 0.4985\n",
      "Epoch 198  Train Loss: 0.4154  Val Loss: 0.4981\n",
      "Epoch 199  Train Loss: 0.4171  Val Loss: 0.4982\n",
      "Epoch 200  Train Loss: 0.4118  Val Loss: 0.4983\n",
      "Epoch 201  Train Loss: 0.4086  Val Loss: 0.4979\n",
      "Epoch 202  Train Loss: 0.4143  Val Loss: 0.4987\n",
      "Epoch 203  Train Loss: 0.4111  Val Loss: 0.4984\n",
      "Epoch 204  Train Loss: 0.4044  Val Loss: 0.4983\n",
      "Epoch 205  Train Loss: 0.4096  Val Loss: 0.4980\n",
      "Epoch 206  Train Loss: 0.4068  Val Loss: 0.4983\n",
      "Epoch 207  Train Loss: 0.4070  Val Loss: 0.4983\n",
      "Epoch 208  Train Loss: 0.4108  Val Loss: 0.4990\n",
      "Epoch 209  Train Loss: 0.4076  Val Loss: 0.4980\n",
      "Epoch 210  Train Loss: 0.4067  Val Loss: 0.4993\n",
      "Epoch 211  Train Loss: 0.4093  Val Loss: 0.5000\n",
      "Epoch 212  Train Loss: 0.4071  Val Loss: 0.4981\n",
      "Epoch 213  Train Loss: 0.4053  Val Loss: 0.4976\n",
      "Epoch 214  Train Loss: 0.4040  Val Loss: 0.4998\n",
      "Epoch 215  Train Loss: 0.4075  Val Loss: 0.4982\n",
      "Epoch 216  Train Loss: 0.4056  Val Loss: 0.4987\n",
      "Epoch 217  Train Loss: 0.4033  Val Loss: 0.4992\n",
      "Epoch 218  Train Loss: 0.4010  Val Loss: 0.4997\n",
      "Epoch 219  Train Loss: 0.4070  Val Loss: 0.4989\n",
      "Epoch 220  Train Loss: 0.3983  Val Loss: 0.4987\n",
      "Epoch 221  Train Loss: 0.4024  Val Loss: 0.4990\n",
      "Epoch 222  Train Loss: 0.4021  Val Loss: 0.4981\n",
      "Epoch 223  Train Loss: 0.4035  Val Loss: 0.4993\n",
      "Epoch 224  Train Loss: 0.4026  Val Loss: 0.4983\n",
      "Epoch 225  Train Loss: 0.4013  Val Loss: 0.4989\n",
      "Epoch 226  Train Loss: 0.4020  Val Loss: 0.4986\n",
      "Epoch 227  Train Loss: 0.3998  Val Loss: 0.4983\n",
      "Epoch 228  Train Loss: 0.4050  Val Loss: 0.4991\n",
      "Epoch 229  Train Loss: 0.4010  Val Loss: 0.4990\n",
      "Epoch 230  Train Loss: 0.3999  Val Loss: 0.4983\n",
      "Epoch 231  Train Loss: 0.4000  Val Loss: 0.4997\n",
      "Epoch 232  Train Loss: 0.3930  Val Loss: 0.4993\n",
      "Epoch 233  Train Loss: 0.3981  Val Loss: 0.4993\n",
      "Epoch 234  Train Loss: 0.3936  Val Loss: 0.4989\n",
      "Epoch 235  Train Loss: 0.4010  Val Loss: 0.4990\n",
      "Epoch 236  Train Loss: 0.3981  Val Loss: 0.5006\n",
      "Epoch 237  Train Loss: 0.3992  Val Loss: 0.4996\n",
      "Epoch 238  Train Loss: 0.3986  Val Loss: 0.4996\n",
      "Epoch 239  Train Loss: 0.3996  Val Loss: 0.4988\n",
      "Epoch 240  Train Loss: 0.3984  Val Loss: 0.5002\n",
      "Epoch 241  Train Loss: 0.3934  Val Loss: 0.4997\n",
      "Epoch 242  Train Loss: 0.3932  Val Loss: 0.4999\n",
      "Epoch 243  Train Loss: 0.3945  Val Loss: 0.5002\n",
      "Epoch 244  Train Loss: 0.3925  Val Loss: 0.5003\n",
      "Epoch 245  Train Loss: 0.3975  Val Loss: 0.4999\n",
      "Epoch 246  Train Loss: 0.3955  Val Loss: 0.5003\n",
      "Epoch 247  Train Loss: 0.3935  Val Loss: 0.5003\n",
      "Epoch 248  Train Loss: 0.3892  Val Loss: 0.4999\n",
      "Epoch 249  Train Loss: 0.3901  Val Loss: 0.5008\n",
      "Epoch 250  Train Loss: 0.3926  Val Loss: 0.4996\n",
      "Epoch 251  Train Loss: 0.3926  Val Loss: 0.4997\n",
      "Epoch 252  Train Loss: 0.3922  Val Loss: 0.5005\n",
      "Epoch 253  Train Loss: 0.3910  Val Loss: 0.4998\n",
      "Epoch 254  Train Loss: 0.3906  Val Loss: 0.5005\n",
      "Epoch 255  Train Loss: 0.3905  Val Loss: 0.5009\n",
      "Epoch 256  Train Loss: 0.3877  Val Loss: 0.5013\n",
      "Epoch 257  Train Loss: 0.3907  Val Loss: 0.5001\n",
      "Epoch 258  Train Loss: 0.3885  Val Loss: 0.5009\n",
      "Epoch 259  Train Loss: 0.3899  Val Loss: 0.5000\n",
      "Epoch 260  Train Loss: 0.3903  Val Loss: 0.5002\n",
      "Epoch 261  Train Loss: 0.3877  Val Loss: 0.5011\n",
      "Epoch 262  Train Loss: 0.3893  Val Loss: 0.5008\n",
      "Epoch 263  Train Loss: 0.3882  Val Loss: 0.5010\n",
      "Epoch 264  Train Loss: 0.3831  Val Loss: 0.5019\n",
      "Epoch 265  Train Loss: 0.3860  Val Loss: 0.5003\n",
      "Epoch 266  Train Loss: 0.3860  Val Loss: 0.5009\n",
      "Epoch 267  Train Loss: 0.3863  Val Loss: 0.5011\n",
      "Epoch 268  Train Loss: 0.3854  Val Loss: 0.5010\n",
      "Epoch 269  Train Loss: 0.3865  Val Loss: 0.4999\n",
      "Epoch 270  Train Loss: 0.3840  Val Loss: 0.5004\n",
      "Epoch 271  Train Loss: 0.3846  Val Loss: 0.5002\n",
      "Epoch 272  Train Loss: 0.3861  Val Loss: 0.5007\n",
      "Epoch 273  Train Loss: 0.3818  Val Loss: 0.5008\n",
      "Epoch 274  Train Loss: 0.3834  Val Loss: 0.5009\n",
      "Epoch 275  Train Loss: 0.3847  Val Loss: 0.5017\n",
      "Epoch 276  Train Loss: 0.3792  Val Loss: 0.5023\n",
      "Epoch 277  Train Loss: 0.3817  Val Loss: 0.5016\n",
      "Epoch 278  Train Loss: 0.3853  Val Loss: 0.5015\n",
      "Epoch 279  Train Loss: 0.3819  Val Loss: 0.5015\n",
      "Epoch 280  Train Loss: 0.3831  Val Loss: 0.5020\n",
      "Epoch 281  Train Loss: 0.3786  Val Loss: 0.5016\n",
      "Epoch 282  Train Loss: 0.3800  Val Loss: 0.5027\n",
      "Epoch 283  Train Loss: 0.3752  Val Loss: 0.5021\n",
      "Epoch 284  Train Loss: 0.3792  Val Loss: 0.5015\n",
      "Epoch 285  Train Loss: 0.3797  Val Loss: 0.5019\n",
      "Epoch 286  Train Loss: 0.3814  Val Loss: 0.5027\n",
      "Epoch 287  Train Loss: 0.3811  Val Loss: 0.5037\n",
      "Epoch 288  Train Loss: 0.3807  Val Loss: 0.5023\n",
      "Epoch 289  Train Loss: 0.3822  Val Loss: 0.5030\n",
      "Epoch 290  Train Loss: 0.3772  Val Loss: 0.5027\n",
      "Epoch 291  Train Loss: 0.3743  Val Loss: 0.5019\n",
      "Epoch 292  Train Loss: 0.3776  Val Loss: 0.5039\n",
      "Epoch 293  Train Loss: 0.3762  Val Loss: 0.5026\n",
      "Epoch 294  Train Loss: 0.3754  Val Loss: 0.5038\n",
      "Epoch 295  Train Loss: 0.3774  Val Loss: 0.5034\n",
      "Epoch 296  Train Loss: 0.3772  Val Loss: 0.5040\n",
      "Epoch 297  Train Loss: 0.3736  Val Loss: 0.5031\n",
      "Epoch 298  Train Loss: 0.3746  Val Loss: 0.5020\n",
      "Epoch 299  Train Loss: 0.3739  Val Loss: 0.5031\n",
      "Epoch 300  Train Loss: 0.3745  Val Loss: 0.5026\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.04      0.07       270\n",
      "         1.0       0.79      0.97      0.87      1010\n",
      "\n",
      "    accuracy                           0.77      1280\n",
      "   macro avg       0.51      0.50      0.47      1280\n",
      "weighted avg       0.67      0.77      0.70      1280\n",
      "\n",
      "Test ROC AUC: 0.4991089108910891\n"
     ]
    }
   ],
   "source": [
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=gemma_train_embs.shape[1]).to(device)\n",
    "\n",
    "# 4) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# 5) Training & Validation Loop\n",
    "n_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # -- Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device).float()\n",
    "        yb = yb.to(device).unsqueeze(-1).float()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # -- Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device).float()\n",
    "            yb = yb.to(device).unsqueeze(-1).float()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 6) Load best model and test evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_probs = []\n",
    "y_true  = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device).float()\n",
    "        yb = yb.to(device).unsqueeze(-1).float()\n",
    "        probs = model(xb)\n",
    "        y_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "        y_true .extend(yb.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_pred = (np.array(y_probs) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3cb288f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aligned_embs= [gemma_train_aligned, gemma_train_aligned, gemma_train_aligned]\n",
    "val_aligned_embs= [gemma_val_aligned, gemma_val_aligned, gemma_val_aligned]\n",
    "test_aligned_embs= [gemma_test_aligned, gemma_test_aligned, gemma_test_aligned]\n",
    "\n",
    "train_ds = EmbeddingMeanDataset(train_aligned_embs, y_train)\n",
    "val_ds = EmbeddingMeanDataset(val_aligned_embs, y_val)\n",
    "test_ds = EmbeddingMeanDataset(test_aligned_embs, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "65151e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = gemma_train_embs.shape[1]\n",
    "c = 1\n",
    "\n",
    "model = nn.Linear(input_dim, c)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c85047a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from scipy import sparse\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    SelectFromModel,\n",
    "    RFE\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c99e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402f50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(npz_paths: str, data_type: str, threshold=0.5, center=0):\n",
    "    upper_threshold = threshold + center\n",
    "    lower_threshold = -threshold + center\n",
    "    if data_type not in [\"X_mean\", \"X_cls\"]:\n",
    "        raise Exception(\"data type in valid\")\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for npz_path in npz_paths:\n",
    "    \n",
    "        base = os.path.splitext(os.path.basename(npz_path))[0]      \n",
    "        csv_path = os.path.join(\n",
    "            os.path.dirname(npz_path),\n",
    "            base + \"_meta.csv\"                     \n",
    "        )\n",
    "        csv_path2 = os.path.join(\n",
    "            os.path.dirname(npz_path),\n",
    "            # base + \"_meta.csv\"    \n",
    "            \"_\".join(base.split(\"_\")[:4]) + \"_features_meta.csv\"                                 \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        X = data[data_type]      # (N_docs, 2*D)\n",
    "        # X_concat = data[\"X_mean\"]\n",
    "        tids = data[\"transcriptids\"]    \n",
    "\n",
    "\n",
    "        meta = pd.read_csv(csv_path)\n",
    "        meta2 = pd.read_csv(csv_path2)\n",
    "\n",
    "        filter_tids = set(meta2.transcriptid)\n",
    "        meta = meta[meta.transcriptid.isin(filter_tids)]\n",
    "\n",
    "        meta_unique = (\n",
    "            meta[[\"transcriptid\", \"SUESCORE\", \"label\"]]\n",
    "            .drop_duplicates(subset=\"transcriptid\", keep=\"first\")\n",
    "            .set_index(\"transcriptid\")\n",
    "        )\n",
    "\n",
    "        mask_ids = np.isin(tids, meta_unique.index)\n",
    "        X_filt = X[mask_ids]\n",
    "        tids_filt = np.array(tids)[mask_ids]\n",
    "\n",
    "\n",
    "        lab_df = meta.assign(\n",
    "            label=lambda df: df.SUESCORE.map(\n",
    "                lambda s: 1 if s >= upper_threshold else (0 if s <= lower_threshold else np.nan)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # meta['label'] = np.nan\n",
    "\n",
    "        # set label = 1 where SUESCORE > threshold\n",
    "        meta.loc[meta['SUESCORE'] >= upper_threshold, 'label'] = 1\n",
    "\n",
    "        # set label = 0 where SUESCORE < -threshold\n",
    "        meta.loc[meta['SUESCORE'] <= lower_threshold, 'label'] = 0\n",
    "\n",
    "        mask_label = lab_df.label.notna().values\n",
    "        # apply the same mask in the same order as the CSV, so we use .loc on lab_df\n",
    "        # but first filter lab_df to only those transcriptids in tids_filt\n",
    "        Xc, y = X_filt[mask_label], meta.loc[mask_label, \"label\"].astype(int).values\n",
    "        \n",
    "        # now align X and y\n",
    "        # X_final = X_filt[lab_sub.label.notna()]\n",
    "        # y_final = lab_sub.label.astype(int).values\n",
    "\n",
    "        # collect\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(y)\n",
    "\n",
    "    # 2. concatenate all files together\n",
    "    Xc = np.vstack(X_list)   # shape: (sum_i N_i, 2*D)\n",
    "    y  = np.concatenate(y_list)  # shape: (sum_i N_i,)\n",
    "\n",
    "    print(\"Combined Xc shape:\", Xc.shape)\n",
    "    print(\"Combined y shape: \", y.shape)\n",
    "\n",
    "    return Xc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d593fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_balance(Xc_unbalanced, y_unbalanced):\n",
    "    # forced resampling\n",
    "    idx0 = np.where(y_unbalanced == 0)[0]\n",
    "    idx1 = np.where(y_unbalanced == 1)[0]\n",
    "\n",
    "    n = min(len(idx0), len(idx1))\n",
    "\n",
    "    sel0 = np.random.choice(idx0, size=n, replace=False)\n",
    "    sel1 = np.random.choice(idx1, size=n, replace=False)\n",
    "\n",
    "    sel = np.concatenate([sel0, sel1])\n",
    "    np.random.shuffle(sel)\n",
    "\n",
    "    # slice out your balanced subset\n",
    "    Xc_out = Xc_unbalanced[sel]\n",
    "    y_out = y_unbalanced[sel]\n",
    "\n",
    "    print(\"Balanced X shape:\", Xc_out.shape)\n",
    "    print(\"Balanced y counts:\", np.bincount(y_out))\n",
    "    return Xc_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421cd270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Xc shape: (4821, 2304)\n",
      "Combined y shape:  (4821,)\n",
      "Combined Xc shape: (2508, 2304)\n",
      "Combined y shape:  (2508,)\n"
     ]
    }
   ],
   "source": [
    "train_npz_paths = [\n",
    "    \"./data/doc_features/gemma_2b_cls_filtered/transcript_componenttext_2012_1_cls_mean.npz\",    \n",
    "    \"./data/doc_features/gemma_2b_cls_filtered/transcript_componenttext_2012_2_cls_mean.npz\",   \n",
    "    \"./data/doc_features/gemma_2b_cls_filtered/transcript_componenttext_2013_1_cls_mean.npz\",   \n",
    "    \"./data/doc_features/gemma_2b_cls_filtered/transcript_componenttext_2013_2_cls_mean.npz\",   \n",
    "]\n",
    "\n",
    "val_npz_paths = [\n",
    "    \"./data/doc_features/gemma_2b_cls_filtered/transcript_componenttext_2014_1_cls_mean.npz\",   \n",
    "    \"./data/doc_features/gemma_2b_cls_filtered/transcript_componenttext_2014_2_cls_mean.npz\", \n",
    "]\n",
    "\n",
    "gemma_train_embs, y_train = load_data(train_npz_paths, \"X_mean\", threshold=0.5)\n",
    "gemma_val_embs, y_val = load_data(val_npz_paths, \"X_mean\", threshold=0.5)\n",
    "\n",
    "# Xc, y = load_data(train_npz_paths, \"X_concat\", threshold=0.1)\n",
    "# X_val_all_feat, y_val = load_data(val_npz_paths, \"X_concat\", threshold=0.1)\n",
    "# X_test_all_feat, y_test = load_data(test_npz_paths, \"X_mean\")\n",
    "# Split X_val_all_feat, y_val into two equal parts:\n",
    "gemma_val_embs, gemma_test_embs, y_val, y_test = train_test_split(\n",
    "    gemma_val_embs, \n",
    "    y_val, \n",
    "    test_size=0.5,       # puts half into X_test/y_test\n",
    "    random_state=42,     # for reproducibility\n",
    "    # stratify=y_val       # if you want to preserve class proportions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4a50962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Xc shape: (4821, 3072)\n",
      "Combined y shape:  (4821,)\n",
      "Combined Xc shape: (2508, 3072)\n",
      "Combined y shape:  (2508,)\n"
     ]
    }
   ],
   "source": [
    "train_npz_paths = [\n",
    "    \"./data/doc_features/llama3_3b_cls_filtered/transcript_componenttext_2012_1_cls_mean.npz\",    \n",
    "    \"./data/doc_features/llama3_3b_cls_filtered/transcript_componenttext_2012_2_cls_mean.npz\",   \n",
    "    \"./data/doc_features/llama3_3b_cls_filtered/transcript_componenttext_2013_1_cls_mean.npz\",   \n",
    "    \"./data/doc_features/llama3_3b_cls_filtered/transcript_componenttext_2013_2_cls_mean.npz\",   \n",
    "]\n",
    "\n",
    "val_npz_paths = [\n",
    "    \"./data/doc_features/llama3_3b_cls_filtered/transcript_componenttext_2014_1_cls_mean.npz\",   \n",
    "    \"./data/doc_features/llama3_3b_cls_filtered/transcript_componenttext_2014_2_cls_mean.npz\", \n",
    "]\n",
    "\n",
    "llama_train_embs, y_train = load_data(train_npz_paths, \"X_mean\", threshold=0.5)\n",
    "llama_val_embs, y_val = load_data(val_npz_paths, \"X_mean\", threshold=0.5)\n",
    "\n",
    "# Xc, y = load_data(train_npz_paths, \"X_concat\", threshold=0.1)\n",
    "# X_val_all_feat, y_val = load_data(val_npz_paths, \"X_concat\", threshold=0.1)\n",
    "# X_test_all_feat, y_test = load_data(test_npz_paths, \"X_mean\")\n",
    "# Split X_val_all_feat, y_val into two equal parts:\n",
    "llama_val_embs, llama_test_embs, y_val, y_test = train_test_split(\n",
    "    llama_val_embs, \n",
    "    y_val, \n",
    "    test_size=0.5,       # puts half into X_test/y_test\n",
    "    random_state=42,     # for reproducibility\n",
    "    # stratify=y_val       # if you want to preserve class proportions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4214b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Xc shape: (4821, 2560)\n",
      "Combined y shape:  (4821,)\n",
      "Combined Xc shape: (2508, 2560)\n",
      "Combined y shape:  (2508,)\n"
     ]
    }
   ],
   "source": [
    "train_npz_paths = [\n",
    "    \"./data/doc_features/qwen_4b_cls_filtered/transcript_componenttext_2012_1_cls_mean.npz\",    \n",
    "    \"./data/doc_features/qwen_4b_cls_filtered/transcript_componenttext_2012_2_cls_mean.npz\",   \n",
    "    \"./data/doc_features/qwen_4b_cls_filtered/transcript_componenttext_2013_1_cls_mean.npz\",   \n",
    "    \"./data/doc_features/qwen_4b_cls_filtered/transcript_componenttext_2013_2_cls_mean.npz\",   \n",
    "]\n",
    "\n",
    "val_npz_paths = [\n",
    "    \"./data/doc_features/qwen_4b_cls_filtered/transcript_componenttext_2014_1_cls_mean.npz\",   \n",
    "    \"./data/doc_features/qwen_4b_cls_filtered/transcript_componenttext_2014_2_cls_mean.npz\", \n",
    "]\n",
    "\n",
    "qwen_train_embs, y_train = load_data(train_npz_paths, \"X_mean\", threshold=0.5)\n",
    "qwen_val_embs, y_val = load_data(val_npz_paths, \"X_mean\", threshold=0.5)\n",
    "\n",
    "# Xc, y = load_data(train_npz_paths, \"X_concat\", threshold=0.1)\n",
    "# X_val_all_feat, y_val = load_data(val_npz_paths, \"X_concat\", threshold=0.1)\n",
    "# X_test_all_feat, y_test = load_data(test_npz_paths, \"X_mean\")\n",
    "# Split X_val_all_feat, y_val into two equal parts:\n",
    "qwen_val_embs, qwen_test_embs, y_val, y_test = train_test_split(\n",
    "    qwen_val_embs, \n",
    "    y_val, \n",
    "    test_size=0.5,       # puts half into X_test/y_test\n",
    "    random_state=42,     # for reproducibility\n",
    "    # stratify=y_val       # if you want to preserve class proportions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d54fe15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_transformation(X_train, target_train, X_test, target_test, X_val=None, use_skl=True, reg=10.0):\n",
    "    # train T: llama -> Gemma\n",
    "    scaler_X = StandardScaler().fit(X_train)\n",
    "    scaler_Y = StandardScaler().fit(target_train)\n",
    "\n",
    "    X_tr = scaler_X.transform(X_train)\n",
    "    Y_tr = scaler_Y.transform(target_train)\n",
    "    X_te = scaler_X.transform(X_test)\n",
    "    Y_te = scaler_Y.transform(target_test)\n",
    "\n",
    "    if X_val is not None:\n",
    "        X_va = scaler_X.transform(X_val)\n",
    "        \n",
    "    alpha = reg\n",
    "    d_X = X_tr.shape[1]\n",
    "\n",
    "    if not use_skl:\n",
    "        I = np.eye(d_X)\n",
    "        # compute W_closed:\n",
    "        W_closed = np.linalg.inv(X_tr.T @ X_tr + alpha * I) @ X_tr.T @ Y_tr\n",
    "\n",
    "        # Map test embeddings and inverse-transform:\n",
    "        Y_tr_from_transform_scaled = X_tr @ W_closed\n",
    "        \n",
    "        if X_val is not None:\n",
    "            Y_val_from_transform_scaled = X_va @ W_closed\n",
    "\n",
    "        Y_pred_scaled = X_te @ W_closed\n",
    "        Y_pred = scaler_Y.inverse_transform(Y_pred_scaled)\n",
    "\n",
    "        # Evaluate (e.g. MSE or cosine similarity)\n",
    "        mse = np.mean((Y_pred - scaler_Y.inverse_transform(Y_te))**2)\n",
    "        print(f\"Closed-form ridge MSE: {mse:.4f}\")\n",
    "        print(np.diag(cosine_similarity(Y_pred_scaled, Y_te)).mean())\n",
    "\n",
    "        if X_val is not None:\n",
    "            return W_closed, Y_tr_from_transform_scaled, Y_val_from_transform_scaled, Y_pred_scaled\n",
    "        else:\n",
    "            return W_closed, Y_tr_from_transform_scaled, Y_pred_scaled\n",
    "        \n",
    "    else:\n",
    "        # 3b) Using sklearn.Ridge:\n",
    "        # Note: sklearn’s Ridge solves for each output dimension jointly when Y is 2D.\n",
    "        model = Ridge(alpha=alpha, fit_intercept=False, solver=\"auto\")# intercept is already handled by StandardScaler\n",
    "        model.fit(X_tr, Y_tr)\n",
    "        W_sklearn = model.coef_.T   \n",
    "\n",
    "        Y_tr_from_transform_scaled = model.predict(X_tr)\n",
    "\n",
    "        if X_val is not None:\n",
    "            Y_val_from_transform_scaled = model.predict(X_va)\n",
    "        \n",
    "        Y_pred_scaled = model.predict(X_te)\n",
    "\n",
    "        Y_pred = scaler_Y.inverse_transform(Y_pred_scaled)\n",
    "        mse = np.mean((Y_pred - scaler_Y.inverse_transform(Y_te))**2)\n",
    "        print(f\"sklearn.Ridge MSE: {mse:.4f}\")\n",
    "        print(np.diag(cosine_similarity(Y_pred_scaled, Y_te)).mean())\n",
    "\n",
    "        if X_val is not None:\n",
    "            return model, Y_tr_from_transform_scaled, Y_val_from_transform_scaled, Y_pred_scaled\n",
    "        else:\n",
    "            return model, Y_tr_from_transform_scaled, Y_pred_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9f59987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airlay88/emb-transfer/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:213: LinAlgWarning: Ill-conditioned matrix (rcond=5.72943e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.Ridge MSE: 0.1413\n",
      "0.87713283\n",
      "Closed-form ridge MSE: 0.1618\n",
      "0.8529351786157778\n"
     ]
    }
   ],
   "source": [
    "trans_llama_gemma, llama_train_aligned, llama_val_aligned, llama_test_aligned = fit_linear_transformation(llama_train_embs, gemma_train_embs, llama_test_embs, gemma_test_embs, llama_val_embs, use_skl=True, reg=2)\n",
    "trans_qwen_gemma, qwen_train_aligned, qwen_val_aligned, qwen_test_aligned = fit_linear_transformation(qwen_train_embs, gemma_train_embs, qwen_test_embs, gemma_test_embs, qwen_val_embs, use_skl=False, reg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ac69887",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(gemma_train_embs)\n",
    "gemma_train_aligned = scaler.transform(gemma_train_embs)\n",
    "gemma_val_aligned = scaler.transform(gemma_val_embs)\n",
    "gemma_test_aligned = scaler.transform(gemma_test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6f350fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aligned_embs = np.array([gemma_train_aligned, llama_train_aligned, qwen_train_aligned]).mean(axis=0)\n",
    "val_aligned_embs = np.array([gemma_val_aligned, llama_val_aligned, qwen_val_aligned]).mean(axis=0)\n",
    "test_aligned_embs = np.array([gemma_test_aligned, llama_test_aligned, qwen_test_aligned]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dfaa751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best C (inverse reg. strength): 0.005\n",
      "CV ROC AUC: 0.655509848358663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3261    0.1807    0.2326       249\n",
      "           1     0.8172    0.9075    0.8600      1005\n",
      "\n",
      "    accuracy                         0.7632      1254\n",
      "   macro avg     0.5716    0.5441    0.5463      1254\n",
      "weighted avg     0.7197    0.7632    0.7354      1254\n",
      "\n",
      "Test ROC AUC: 0.6285839876920618\n"
     ]
    }
   ],
   "source": [
    "# X_mean, l2 0.005\n",
    "param_grid = {\"logisticregression__C\": [0.0001, 0.005, 0.1]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(gemma_train_aligned, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(gemma_test_aligned)[:, 1]\n",
    "y_pred       = best_clf.predict(gemma_test_aligned)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00ef463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best C (inverse reg. strength): 0.005\n",
      "CV ROC AUC: 0.6575908618991767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2945    0.1727    0.2177       249\n",
      "           1     0.8141    0.8975    0.8538      1005\n",
      "\n",
      "    accuracy                         0.7536      1254\n",
      "   macro avg     0.5543    0.5351    0.5357      1254\n",
      "weighted avg     0.7109    0.7536    0.7275      1254\n",
      "\n",
      "Test ROC AUC: 0.6278966612719535\n"
     ]
    }
   ],
   "source": [
    "# X_mean, l2 0.005\n",
    "param_grid = {\"logisticregression__C\": [0.0001, 0.005, 0.1]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(llama_train_aligned, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(llama_test_aligned)[:, 1]\n",
    "y_pred       = best_clf.predict(llama_test_aligned)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfa2b84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best C (inverse reg. strength): 0.005\n",
      "CV ROC AUC: 0.6580540221129143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2886    0.1727    0.2161       249\n",
      "           1     0.8136    0.8945    0.8521      1005\n",
      "\n",
      "    accuracy                         0.7512      1254\n",
      "   macro avg     0.5511    0.5336    0.5341      1254\n",
      "weighted avg     0.7093    0.7512    0.7258      1254\n",
      "\n",
      "Test ROC AUC: 0.6194209674518971\n"
     ]
    }
   ],
   "source": [
    "# X_mean, l2 0.005\n",
    "param_grid = {\"logisticregression__C\": [0.0001, 0.005, 0.1]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(qwen_train_aligned, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(qwen_test_aligned)[:, 1]\n",
    "y_pred       = best_clf.predict(qwen_test_aligned)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28e65f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best C (inverse reg. strength): 0.005\n",
      "CV ROC AUC: 0.6590355375682642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3359    0.1727    0.2281       249\n",
      "           1     0.8171    0.9154    0.8634      1005\n",
      "\n",
      "    accuracy                         0.7679      1254\n",
      "   macro avg     0.5765    0.5441    0.5458      1254\n",
      "weighted avg     0.7215    0.7679    0.7373      1254\n",
      "\n",
      "Test ROC AUC: 0.6373154308777398\n"
     ]
    }
   ],
   "source": [
    "# X_mean, l2 0.005\n",
    "param_grid = {\"logisticregression__C\": [0.0001, 0.005, 0.1]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(train_aligned_embs, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(test_aligned_embs)[:, 1]\n",
    "y_pred       = best_clf.predict(test_aligned_embs)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12854f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
